<a id='TOC'></a>
</div>

<div style=" border: 3px solid #13B3E7; border-radius:10px; font-size:115%;">


<h2 style="padding-left:15px">Table of Content</h2>
<ul>
    



<li 
    style="font: 14pt 'Candara'; margin-bottom: 6px;font-weight:bold;">
        <a href="#I">1Ÿê. Introduction</a>
    <ul style="font: 12pt 'Candara'; margin-bottom: 6px; color:green;">
        <p href="#"> </p></ul>
    <ul style="font: 12pt 'Candara'; margin-bottom: 6px;;">
        <a href="#A"> 1.1 Notations</a></ul>
    <ul style="font: 12pt 'Candara'; margin-bottom: 6px; color:green;">
        <p href="#"> </p></ul>
    </li>
<li 
    style="font: 14pt 'Candara'; margin-bottom: 6px;font-weight:bold;">
        <a href="#SL">2.  Statistical Learning</a>
    <ul style="font: 12pt 'Candara'; margin-bottom: 6px; color:green;">
        <p href="#"> </p></ul>
    <ul style="font: 12pt 'Candara'; margin-bottom: 6px;;">
        <a href="#2.1">2.1 Data load and plots</a></ul>
    <ul style="font: 12pt 'Candara'; margin-bottom: 6px;;">
        <a href="#2.1.1">2.1.1  f (X) => Function of X</a></ul>
    <ul style="font: 12pt 'Candara'; margin-bottom: 6px;;">
        <a href="#2.1.2">2.1.2 Parametric and Non-Parametric Methods</a></ul>
    <ul style="font: 12pt 'Candara'; margin-bottom: 6px;;">
        <a href="#2.1.4">2.1.4 Supervised VS Unsupervised Learning</a></ul>
    <ul style="font: 12pt 'Candara'; margin-bottom: 6px;;">
        <a href="#2.1.5">2.1.5 Regression VS Classifcation Problems</a></ul>
    <ul style="font: 12pt 'Candara'; margin-bottom: 6px;;">
        <a href="#2.2">2.2 Assessing Model Accuracy</a></ul>
    <ul style="font: 12pt 'Candara'; margin-bottom: 6px;;">
        <a href="#2.2.1">2.2.1 Measuring the Quality of Fit (MSE)</a></ul>
    <ul style="font: 12pt 'Candara'; margin-bottom: 6px;;">
        <a href="#2.2.2">2.2.2 The Bias-Variance Trade-Of</a></ul>
    <ul style="font: 12pt 'Candara'; margin-bottom: 6px;;">
        <a href="#2.2.3">2.2.3 The Classifcation Setting</a></ul>
      <ul style="font: 12pt 'Candara'; margin-bottom: 6px; color:green;">    
        <p href="#"> </p></ul>
    </li>

<!-- <li 
    style="font: 14pt 'Candara'; margin-bottom: 6px;font-weight:bold;">
        <a href="#SL">3.  Python</a>
    <ul style="font: 12pt 'Candara'; margin-bottom: 6px; color:green;">
        <p href="#"> </p></ul>
    </li> -->

<li 
    style="font: 14pt 'Candara'; margin-bottom: 6px;font-weight:bold;">
        <a href="#LR">3.  Linear Regression</a>
    <ul style="font: 12pt 'Candara'; margin-bottom: 6px; color:green;">
        <p href="#"> </p></ul>
    <ul style="font: 12pt 'Candara'; margin-bottom: 6px;;">
        <a href="#3.1">3.1 Simple Linear Regression</a></ul>
    <ul style="font: 12pt 'Candara'; margin-bottom: 6px;;">
        <a href="#3.1.1"> 3.1.1 Estimating the Coefficients </a></ul>
    <ul style="font: 12pt 'Candara'; margin-bottom: 6px;;">
        <a href="#3.1.2"> 3.1.2 AssessingtheAccuracyof theCoefficientEstimates</a></ul>
    <ul style="font: 12pt 'Candara'; margin-bottom: 6px;;">
        <a href="#3.1.3"> 3.1.3 Assessing the Accuracy of the Model</a></ul>
    <ul style="font: 12pt 'Candara'; margin-bottom: 6px;;">
        <a href="#3.2"> 3.2 Multiple Linear Regression</a></ul>
    <ul style="font: 12pt 'Candara'; margin-bottom: 6px;;">
        <a href="#3.2.1"> 3.2.1 Estimating the Regression Coefficients</a></ul>
    <ul style="font: 12pt 'Candara'; margin-bottom: 6px;;">
        <a href="#3.2.2"> 3.2.2 Some Important Questions</a></ul>
    <ul style="font: 12pt 'Candara'; margin-bottom: 6px;;">
        <a href="#3.3"> 3.3 Other Considerations in the Regression Model</a></ul>
    <ul style="font: 12pt 'Candara'; margin-bottom: 6px;;">
        <a href="#3.3.1"> 3.3.1 Qualitative Predictors</a></ul>
    <ul style="font: 12pt 'Candara'; margin-bottom: 6px;;">
        <a href="#3.3.2">  3.3.2 Extensions of the Linear Model</a></ul>
    <ul style="font: 12pt 'Candara'; margin-bottom: 6px;;">
        <a href="#3.3.3">  3.3.3 PotentialProblems</a></ul>
    <ul style="font: 12pt 'Candara'; margin-bottom: 6px;;">
        <a href="#3.4">  3.4 The Marketing Plan</a></ul>
      <ul style="font: 12pt 'Candara'; margin-bottom: 6px; color:green;">    
        <a href="#3.5">   3.5 Comparison of Linear Regression with K-Nearest Neighbors</a></ul>
      <ul style="font: 12pt 'Candara'; margin-bottom: 6px; color:green;">    
        <a href="#3.6">   3.6 Lab: Linear Regression</a></ul>
      <ul style="font: 12pt 'Candara'; margin-bottom: 6px; color:green;">    
        <p href="#"> </p></ul>
    </li>
  




<li 
    style="font: 14pt 'Candara'; margin-bottom: 6px;font-weight:bold;">
        <a href="#CL">4.  Classification</a>
    <ul style="font: 12pt 'Candara'; margin-bottom: 6px; color:green;">
        <p href="#"> </p></ul>
    <ul style="font: 12pt 'Candara'; margin-bottom: 6px;;">
        <a href="#4.1">4.1 An Overview of Classification</a></ul>
    <ul style="font: 12pt 'Candara'; margin-bottom: 6px;;">
        <a href="#4.2"> 4.2 Why Not Linear Regression? </a></ul>
    <ul style="font: 12pt 'Candara'; margin-bottom: 6px;;">
        <a href="#4.3"> 4.3 Logistic Regression</a></ul>
    <!-- <ul style="font: 12pt 'Candara'; margin-bottom: 6px;;">
        <a href="#3.1.3"> 3.1.3 Assessing the Accuracy of the Model</a></ul>
    <ul style="font: 12pt 'Candara'; margin-bottom: 6px;;">
        <a href="#3.2"> 3.2 Multiple Linear Regression</a></ul>
    <ul style="font: 12pt 'Candara'; margin-bottom: 6px;;">
        <a href="#3.2.1"> 3.2.1 Estimating the Regression Coefficients</a></ul>
    <ul style="font: 12pt 'Candara'; margin-bottom: 6px;;">
        <a href="#3.2.2"> 3.2.2 Some Important Questions</a></ul>
    <ul style="font: 12pt 'Candara'; margin-bottom: 6px;;">
        <a href="#3.3"> 3.3 Other Considerations in the Regression Model</a></ul>
    <ul style="font: 12pt 'Candara'; margin-bottom: 6px;;">
        <a href="#3.3.1"> 3.3.1 Qualitative Predictors</a></ul>
    <ul style="font: 12pt 'Candara'; margin-bottom: 6px;;">
        <a href="#3.3.2">  3.3.2 Extensions of the Linear Model</a></ul>
    <ul style="font: 12pt 'Candara'; margin-bottom: 6px;;">
        <a href="#3.3.3">  3.3.3 PotentialProblems</a></ul> -->
    <ul style="font: 12pt 'Candara'; margin-bottom: 6px;;">
        <a href="#4.4">  4.4 Generative models for calssification</a></ul>
      <ul style="font: 12pt 'Candara'; margin-bottom: 6px; color:green;">    
        <a href="#4.5">   4.5 Comparison of Classification Methdos</a></ul>
      <ul style="font: 12pt 'Candara'; margin-bottom: 6px; color:green;">    
        <a href="#4.6">   4.6 Generalized Linear Models</a></ul>
      <ul style="font: 12pt 'Candara'; margin-bottom: 6px; color:green;">    
        <p href="#"> </p></ul>
    </li>
  
</ul>

<a id='I'></a>
<div 
     style="border-width:0.05;
            border-radius: 15px;
            border-style: solid;
            border-color: lightblue;
            background-color: white;
            text-align: center;
            font: 14pt 'Candara';
            font-weight:bold;padding-bottom: 15px;
            width: 35%;
            padding:10px;
            margin:0;
            ">
    <h1
        style = " 
                 color: black;
                 text-align: center;
                 font-family: Trebuchet MS;
                 padding:10px;
                 margin:0
                 "> 1.Introduction
    </h1>
</div></h1>
</div>

<a id='A'></a>
<h4 style= "float: right"> <a href="#TOC" >Go to Table of content</a> </h4>

## 1.1 Notations

#### A1. Notation in Math

>**Notation is a common Math language used to communicate Like:**

##### - `X` => Variables "Columns in dataset". (Capital X)

##### - `x` => individual outcoum of variable "Cell in column from dataset". (Lower X)

##### - `p` => the number of variables that are available for use in making predictions.

##### - `n` => the number of observations.

**For example, the Wage data set consists of 11 variables for 3,000 people, so we have n = 3,000 observations and p = 11**

##### - $x_{ij}$ => represent the value of the jth variable for the ith observation, where i = 1, 2,...,n and j = 1, 2,...,p


##### - $x_{i}$ => is a vector of length 11, consisting of year, age, race, **Column.**

##### - `T` => denotes the transpose of a matrix or vector (Transposed matrix)

##### - `y` => Column that we want to predict.

##### - $y_{i}$ => the ith observation of the variable on which we wish to make predictions

### Normal Matrix
\[
#### X = 
 \begin{pmatrix}
x_{11} & x_{12} & \cdots & x_{1p} \\
x_{21} & x_{22} & \cdots & x_{2p} \\
\vdots & \vdots & \ddots & \vdots \\
x_{n1} & x_{n2} & \cdots & x_{np}
\end{pmatrix}
\]


### Transposed Matrix
\[

$X^{T}$ = 
 \begin{pmatrix}
x_{11} & x_{21} & \cdots & x_{n1} \\
x_{12} & x_{22} & \cdots & x_{n2} \\
\vdots & \vdots & \ddots & \vdots \\
x_{1p} & x_{2p} & \cdots & x_{np}
\end{pmatrix},
\]


<h4 style="color:red">----------------------------------------------------------------------------------------------------</h4>

 Age              | Year | Race |
| :---------------- | :------: | ----: |
| 2006        |   18   | Asian |
| 2010           |   16   | White |
| 2018    |  33   | Black |
| 2022 |  25   | Other |



##### in this Table 

##### $x_{1}$ is Age column 

##### $x_{2}$ is Year Column 

#####  $x_{11}$ is 2006 

##### $x_{12}$ is 2010

##### $x_{32}$ is white 

<a id='SL'></a>
<div 
     style="border-width:0.05;
            border-radius: 15px;
            border-style: solid;
            border-color: lightblue;
            background-color: white;
            text-align: center;
            font: 14pt 'Candara';
            font-weight:bold;padding-bottom: 15px;
            width: 50%;
            padding:10px;
            margin:0;
            ">
    <h1
        style = " 
                 color: black;
                 text-align: center;
                 font-family: Trebuchet MS;
                 padding:10px;
                 margin:0
                 "> 2.Statistical Learning
    </h1>
</div></h1>
</div>

<a id='2.1'></a>
<h4 style="color:red">-----------------------------------------------------------------------------------------------------------------------------</h4>

<h4 style= "float: right";> <a href="#TOC" >Go to Table of content</a> </h4>

## 2.1 Data load and plots
<h4 style="color:red">-----------------------------------------------------------------------------------------------------------------------------</h4>



```python
import pandas as pd 
import matplotlib.pyplot as plt
import seaborn as sns
%matplotlib inline
```


```python
advertise = pd.read_csv(r"E:\1_Work\2_Data_CLin\Books\ITSL\Data\Advertising.csv")
advertise.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Unnamed: 0</th>
      <th>TV</th>
      <th>Radio</th>
      <th>Newspaper</th>
      <th>Sales</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>230.1</td>
      <td>37.8</td>
      <td>69.2</td>
      <td>22.1</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2</td>
      <td>44.5</td>
      <td>39.3</td>
      <td>45.1</td>
      <td>10.4</td>
    </tr>
    <tr>
      <th>2</th>
      <td>3</td>
      <td>17.2</td>
      <td>45.9</td>
      <td>69.3</td>
      <td>9.3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4</td>
      <td>151.5</td>
      <td>41.3</td>
      <td>58.5</td>
      <td>18.5</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5</td>
      <td>180.8</td>
      <td>10.8</td>
      <td>58.4</td>
      <td>12.9</td>
    </tr>
  </tbody>
</table>
</div>



#### This is alinear-Regression Line fit separatly for each Feature


```python
fig,ax = plt.subplots(1,3,figsize=(25,8))
sns.regplot(data = advertise,  x = "TV", y="Sales", ax=ax[0])
sns.regplot(data = advertise,  x = "Radio", y="Sales", ax=ax[1], color="orange")
sns.regplot(data = advertise,  x = "Newspaper", y="Sales", ax=ax[2], color="orchid");

```


    
![png](ITSL_files/ITSL_13_0.png)
    


#### We can see the correlation between (TV and sales) and between (Radio and sales) is slightly stronger than correlation between (Newspaper and sales)

#### But, what if want to know the joint relationship between the response variable `Sales` and all features combined (TV, Radio and Newspaper)

**Sales ‚âà f(TV, Radio, Newspaper)**

#### This Problem notations

##### - `Y` => Column that we have and we want to predict.
##### - `Y^` => our  prediction.
##### - `f` => the real function that we want to use to predict. (True Function)
##### - `f^` => the function to predict that we find from our data. (estimated Function)
##### - `e` => captures measurment errors and other discrepancies (errors and things that we can't capture in the function).

X = ($x_{1}$, $x_{2}$, $x_{3}$)


**in This example**

X = (TV, Radio, Newspaper)

Y = f(X) + e

<a id='2.1.1'></a>
<h4 style="color:red">-----------------------------------------------------------------------------------------------------------------------------</h4>
<h4 style= "float: right"> <a href="#TOC" >Go to Table of content</a> </h4>

## 2.1.1  f(X) => Function of X
<h4 style="color:red">-----------------------------------------------------------------------------------------------------------------------------</h4>


<h3 style= "text-align:center">What is f(x) good for </h3>

#### 1. With a good `f` we can make predictions of `Y` at any new points of `x`.

#### 2. We can understand which compomemts of X(X1, X2, X2) are important in explainig `Y` and which are irrelevant 
> **Like: Seniority and years of education have a big impact on Income, But Marital status typically doesn't**

#### 3. Depending on the complexity of `f` we may be able to understand how each component $X_{i}$ of X affects `Y`

### `Y^ = fÀÜ(X)`
> #### The accuracy of YÀÜ as a prediction for Y depends on two quantities

### 1. reducible error

>#### In general, fÀÜ will not be a perfect estimate for f, and this inaccuracy will introduce some error. This error is reducible because we can potentially improve the accuracy of fÀÜ by using the most appropriate statistical learning technique to estimate f

#### For example, (in the risk of an adverse reaction) Incomplete medical data, Missing information on allergies or overly simplistic model can make some errors that we can reduce by fixing missing data or try different methods on trainig our model.



### 1. irreducible error

>#### However, even if it were possible to form a perfect estimate for f, so that our estimated response took the form YÀÜ = f(X), our prediction would still have some error in it! This is because Y is also a function of e, which, by defnition, cannot be predicted using X. Therefore, variability associated with e also afects the accuracy of our predictions. This is known as the irreducible error, because no matter how well we estimate f, we cannot reduce the error introduced by ".


#### For example, the risk of an adverse reaction might vary for a given patient on a given day, depending on manufacturing variation in the drug itself or the patient‚Äôs general feeling of well-being on that day.

\[
\begin{aligned}
\mathbb{E}(Y - \hat{Y})^2 &= \mathbb{E}[f(X) + \epsilon - \hat{f}(X)]^2 \\
&= \underbrace{[f(X) - \hat{f}(X)]^2}_{\text{Reducible}} + \underbrace{\text{Var}(\epsilon)}_{\text{Irreducible}},
\end{aligned}
\]


#### where E(Y ‚àí YÀÜ )2 represents the average, or expected value, of the squared diference between the predicted and actual value of Y , and Var(e) represents the variance associated with the error term e.

#### The focus of this book is on techniques for estimating f with the aim of minimizing the reducible error. It is important to keep in mind that the irreducible error will always provide an upper bound on the accuracy of our prediction for Y . This bound is almost always unknown in practice.

<h2 style= "text-align:center">Easy Explaination of equation </h2>


<h4>The equation E(Y - Y^)**2 represents the expected value of the squared difference between the actual value Y nd the predicted value Y^ . This concept is central to understanding how well a prediction model performs.

<h4 style= "text-align:center">Let's break it down step-by-step </h4>



>-  <h4> Y This is the true value you want to predict. For example, it could be the actual house price, the true temperature, or the real stock price.</h4>

>* <h4>Y^ This is the value your model predicts. It's an estimate based on the input data and the model you have used.</h4>

>* <h4> Y - Y^ This is the error or residual. It shows how much your prediction differs from the actual value. A smaller difference means a better prediction.</h4>

>* <h4>Squared Differenc `Y - Y^` squared By squaring the difference, we ensure that all errors are positive (since squaring a negative number makes it positive) and give more weight to larger errors. This helps in penalizing bigger mistakes more heavily.</h4>

>* <h4> Expected Value `E`  The expected value (or mean) is the average of all possible squared differences. It provides a single measure that summarizes the performance of the model over all predictions..</h4>

<h2 style= "text-align:center">There are 3 Main questions to make predictions </h2>


#### 1.Which predictors are associated with the response?


#### 2.What is the relationship between the response and each predictor?


#### 3.Can the relationship between Y and each predictor be adequately summarized using a linear equation, or is the relationship more complicated?

<h2 style= "text-align:center">Notes </h2>

#### linear models allow for relatively simple and interpretable inference, but may not yield as accurate predictions as some other approaches.


#### some of the highly non-linear approaches can potentially provide quite accurate predictions for Y , but this comes at the expense of a lesssinterpretable model for which inference is more challenging.

<a id='2.1.2'></a>
<h4 style="color:red">-----------------------------------------------------------------------------------------------------------------------------</h4>

<h4 style= "float: right"> <a href="#TOC" >Go to Table of content</a> </h4>

## 2.1.2 Parametric and Non-Parametric Methods
<h4 style="color:red">-----------------------------------------------------------------------------------------------------------------------------</h4>


# Parametric Methods

#### Parametric methods involve a two-step model-based approach.
* 1. First, we make an assumption about the functional form, or shape,
of f. For example, one very simple assumption is that f is linear in
X:
f(X) = Œ≤0 + Œ≤1X1 + Œ≤2X2 + ¬∑¬∑¬∑ + Œ≤pXp

* 2. After a model has been selected, we need a procedure that uses the
training data to ft or train the model. In the case of the linear model we need to estimate the parameters Œ≤0, Œ≤1,..., Œ≤p. That is, we
want to fnd values of these parameters such that
Y ‚âà Œ≤0 + Œ≤1X1 + Œ≤2X2 + ¬∑¬∑¬∑ + Œ≤pXp

#### It has fixed Number of Parameters:
* The number of parameters is fixed and does not change with the size of the training data.
* For example, a linear regression with one input variable always has two parameters: the intercept and the slope.



#### Examples:

Linear regression, logistic regression, and neural networks (with fixed architecture).

#### Pros and Cons:

**Pros: Often simpler and faster to train, and easier to interpret.**


**Cons: May not capture complex relationships if the chosen model form is incorrect.**


<h2 style= "text-align:center">income ‚âà Œ≤0 + Œ≤1 √ó education + Œ≤2 √ó seniority. </h2>


# Non-Parametric Methods

#### Non-parametric methods do not assume a specific form for the relationship between inputs and outputs. Instead, they can adapt to the structure of the data.

#### It has Flexible  Number of Parameters:

* The complexity of the model can grow with the size of the training data. The number of parameters is not fixed and can increase as more data is available.
* The model training involves using the training data directly to make predictions. This can mean storing the training data and using it to make predictions on new data points.


#### Examples:

* k-nearest neighbors (KNN), decision trees, and kernel methods like support vector machines (SVM).

#### Pros and Cons:

**Pros: Can model complex relationships and adapt to the data without a predefined form.**

**Cons: Can be slower to train and predict, and may require more data to make accurate predictions.**



<h2 style= "text-align:center">Summary </h2>

**Parametric Methods: Assume a specific form for the model, have a fixed number of parameters, and are usually simpler and faster but may miss complex patterns.**

**Non-Parametric Methods: Do not assume a specific form, can adapt to the data, and are more flexible but can be slower and require more data.**

<a id='2.1.4'></a>
<h4 style="color:red">-----------------------------------------------------------------------------------------------------------------------------</h4>

<h4 style= "float: right"> <a href="#TOC" >Go to Table of content</a> </h4>

## 2.1.4 Supervised VS Unsupervised Learning
<h4 style="color:red">-----------------------------------------------------------------------------------------------------------------------------</h4>



> ### Supervised Learning

<h4>a type of machine learning where the algorithm is trained on labeled data. This means that each training example is paired with an output label. The goal is for the algorithm to learn a mapping from inputs to outputs so it can predict the output for new, unseen inputs.</h6>


#### Example
<h5>Imagine you are teaching a child to identify fruits. You show the child several pictures of fruits along with their names (labels), like:</h5>

* An apple labeled "apple"
* A banana labeled "banana"
* A cherry labeled "cherry"

**After seeing enough labeled examples, the child learns to recognize and correctly identify the fruits in new pictures.**

#### Use Cases
* <b>Spam Detection:</b> Emails are labeled as "spam" or "not spam." The algorithm learns to classify new emails.
* <b>Image Classification:</b> Photos are labeled with the objects they contain (e.g., cat, dog). The algorithm learns to identify objects in new photos.
* <b>Predictive Maintenance:</b> Machine data is labeled with "working" or "failing." The algorithm predicts when machines might fail.







> ### Unsupervised Learning

<h4>a type of machine learning where the algorithm is trained on unlabeled data. The algorithm tries to learn the underlying structure of the data without explicit instructions on what to predict. The goal is often to find patterns, groupings, or to simplify the data.</h6>


#### Example
<h5>Imagine you have a basket of mixed fruits, but this time you don‚Äôt know their names. You want to sort them into different groups based on their features (like color, size, shape). The child, without any labels, starts grouping the fruits into different clusters:</h5>

* A group of red, round fruits (possibly apples)
* A group of long, yellow fruits (possibly bananas)
* A group of small, red fruits (possibly cherries)


#### Use Cases
* <b>Customer Segmentation:</b> Grouping customers based on purchasing behavior without predefined categories.
* <b>Anomaly Detection:</b> Identifying unusual data points that do not fit into any group (e.g., fraud detection).
* <b>Market Basket Analysis:</b> Finding associations between products bought together without knowing specific patterns beforehand.







> ### Summary



<h4>Supervised Learning:</h4>

* <b>Labeled Data:</b> Yes
* <b>Goal:</b> Predict output for new inputs
* <b>Example:</b> Predicting fruit names based on labeled pictures

<h4>Unsupervised Learning:</h4>

* <b>Labeled Data:</b> Yes
* <b>Goal:</b> Discover patterns or groupings
* <b>Example:</b> Grouping fruits based on features without knowing their names


<a id='2.1.5'></a>
<h4 style="color:red">-----------------------------------------------------------------------------------------------------------------------------</h4>

<h4 style= "float: right"> <a href="#TOC" >Go to Table of content</a> </h4>

## 2.1.5 Regression VS Classifcation Problems
<h4 style="color:red">-----------------------------------------------------------------------------------------------------------------------------</h4>



> ### Regression Problems

<h4>Regression problems are a type of supervised learning where the goal is to predict a continuous numerical value. The output variable is a real number, such as temperature, price, or age.</h6>


#### Example
<h5>Imagine you are a real estate agent and you want to predict the price of a house based on its features like size, number of bedrooms, and location</h5>

* <b>Input Features:</b> Size of the house (in square feet), number of bedrooms, location
* <b>Output</b>Price of the house (in dollars)

**In this scenario, you use historical data of house prices and their features to train a model that can predict the price of a new house based on its features.**

#### Use Cases
* <b>House Price Prediction:</b> Predicting the price of a house based on its features.
* <b>Stock Price Prediction:</b> Predicting the future price of a stock based on historical data.
* <b>Weather Forecasting:</b> Predicting the temperature for the next day based on weather patterns.







> ### Classification Problems

<h4>Classification problems are a type of supervised learning where the goal is to predict a categorical label. The output variable is a category, such as spam/not spam, cat/dog, or disease/healthy.</h6>


#### Example
<h5>Imagine you are working on an email filtering system that needs to classify emails as either "spam" or "not spam."</h5>

* <b>Input Features:</b> Words in the email, frequency of certain words, sender's email address
* <b>Output</b>Spam (1) or Not Spam (0)

**In this case, you train the model with labeled examples of emails that are already categorized as spam or not spam, so the model can learn to classify new emails accordingly.**

#### Use Cases
* <b>Email Spam Detection:</b> Classifying emails as spam or not spam.
* <b>Image Recognition:</b> Identifying whether an image contains a cat or a dog.
* <b>Medical Diagnosis:</b> Classifying whether a patient has a disease based on medical tests.


> ### Summary



<h4>Regression Problems:</h4>

* <b>Goal:</b> Predict a continuous numerical value.
* <b>Output:</b> Real number (e.g., price, temperature).
* <b>Example:</b> Predicting the price of a house.

<h4>Classification Problems:</h4>

* <b>Goal:</b> Predict a categorical label.
* <b>Output:</b> Category (e.g., spam/not spam, cat/dog).
* <b>Example:</b> Classifying an email as spam or not spam.

<a id='2.2'></a>
<h4 style="color:red">-----------------------------------------------------------------------------------------------------------------------------</h4>

<h4 style= "float: right"> <a href="#TOC" >Go to Table of content</a> </h4>

## 2.2 Assessing Model Accuracy
<h4 style="color:red">-----------------------------------------------------------------------------------------------------------------------------</h4>


#### This book aims to introduce a variety of statistical learning methods beyond standard linear regression. The reason for covering many methods is that no single method works best for all data sets. Different methods may perform better on different data sets. Therefore, it's crucial to determine which method works best for a specific data set. Selecting the right method is often one of the most challenging tasks in statistical learning. This section discusses key concepts for choosing the best statistical learning method for a given data set, and as the book progresses, it will show how to apply these concepts in practice.

<a id='2.2.1'></a>
<h4 style="color:red">-----------------------------------------------------------------------------------------------------------------------------</h4>

<h4 style= "float: right"> <a href="#TOC" >Go to Table of content</a> </h4>

## 2.2.1 Measuring the Quality of Fit (MSE)
<h4 style="color:red">-----------------------------------------------------------------------------------------------------------------------------</h4>


#### In order to evaluate the performance of a statistical learning method on a given data set, we need some way to measure how well its predictions actually match the observed data

### In the regression setting, the most commonly-used measure is the mean squared error (MSE) Mean Square Error

MSE = AVG($y_{i}$ - $\hat{y}_i$)**2 

**Average of (actual y - predicted y)squared**

> **The MSE will be small if the predicted responses are very close to the true responses, and will be large if for some of the observations, the predicted and true responses difer substantiall**
> **We should calculate MSE for Both Trainig Data and Testing Data**
> <h4>We can train the method using stock returns from the past
6 months. But we don‚Äôt really care how well our method predicts last week‚Äôs
stock price. We instead care about how well it will predict tomorrow‚Äôs price
or next month‚Äôs price.</h4>

<h3 style="color:red">Note that regardless of whether or not overftting has
occurred, we almost always expect the training MSE to be smaller than
the test MSE because most statistical learning methods either directly or
indirectly seek to minimize the training MSE. 
</h3>


<h3 style="color:red"> Overftting refers specifcally
to the case in which a less fexible model would have yielded a smaller
test MSE.</h3>

<a id='2.2.2'></a>
<h4 style="color:red">-----------------------------------------------------------------------------------------------------------------------------</h4>
<h4 style= "float: right"> <a href="#TOC" >Go to Table of content</a> </h4>

## 2.2.2 The Bias-Variance Trade-Of<h4 style="color:red">-----------------------------------------------------------------------------------------------------------------------------</h4>



<h5>The bias-variance trade-off is a fundamental concept in machine learning and statistics that describes the balance between two sources of error that affect the performance of predictive models: bias and variance.</h5>



> ### Bias

* <h5>Bias is the error introduced by approximating a real-world problem, which may be complex, by a simplified model.</h5>

* <h5>High bias means the model is too simple and fails to capture the underlying patterns of the data, leading to underfitting.</h5>

**<h5>For example, using a straight line to fit a data set that has a complex, curved relationship would result in high bias.</h5>**



> ### Variance

* <h5>Variance is the error introduced by the model's sensitivity to small fluctuations in the training data.</h5>

* <h5>High variance means the model is too complex and captures noise along with the underlying pattern, leading to overfitting.</h5>

**<h5>For example, a model that fits every minor detail in the training data will perform poorly on new, unseen data because it is too tailored to the training data.</h5>**



> ### Trade-Off

<h4>The goal is to find a model that minimizes both bias and variance to achieve good predictive performance on new data. Here's the trade-off:</h4>

* <h5>Low Bias, High Variance: A complex model (like a high-degree polynomial) that fits the training data very well but performs poorly on new data</h5>

* <h5>High Bias, Low Variance: A simple model (like a linear model) that does not capture the complexity of the training data but generalizes better to new data.</h5>

* <h5>Optimal Model: A balanced model that is not too simple (low bias) and not too complex (low variance), providing the best predictive performance.</h5>



> ### Visual Explanation


<h4>Imagine fitting models to a data set:</h4>

* <h5>Underfitting (High Bias): The model is too simple (e.g., a straight line), missing important patterns.</h5>

* <h5>Overfitting (High Variance): The model is too complex (e.g., a wiggly curve), capturing noise rather than the true pattern.</h5>

* <h5>Just Right (Balanced Bias and Variance): The model captures the essential patterns without overreacting to noise.</h5>



> ### Example in Everyday Terms



<h4>Imagine fitting models to a data set:</h4>

* <h5> <b>High Bias:</b> Imagine a poorly trained dart player who always throws darts far from the bullseye but in a consistent pattern. This represents high bias, as the player's throws are consistently off target.</h5>

* <h5> <b>High Variance:</b> Now imagine an unsteady dart player whose darts scatter widely around the board, sometimes hitting the bullseye and other times missing completely. This represents high variance, as the player's throws are highly inconsistent.</h5>

* <h5> <b>Balanced:</b> An experienced dart player who consistently throws darts close to the bullseye represents a model with low bias and low variance, striking a good balance.</h5>


> ### Summary



<h4>Imagine fitting models to a data set:</h4>

* <h5> <b>Bias:</b> Error from overly simple models (underfitting).</h5>

* <h5> <b>Variance:</b>Error from overly complex models (overfitting).</h5>

* <h5> <b>Trade-Off::</b> The challenge is to find a model that balances bias and variance to minimize the overall error and perform well on new data.</h5>

<a id='2.2.3'></a>
<h4 style="color:red">-----------------------------------------------------------------------------------------------------------------------------</h4>
<h4 style= "float: right"> <a href="#TOC" >Go to Table of content</a> </h4>

## 2.2.3 The Classifcation Setting<h4 style="color:red">-----------------------------------------------------------------------------------------------------------------------------</h4>



<h4>This formula is used to calculate the <b style="color:blue">classification error rate</b> in machine learning</h4>

<h5> It calculates the proportion of incorrect predictions out of the total number of predictions, giving us the classification error rate. The lower the error rate, the better the model is at making accurate predictions.</h5>

$$
\frac{1}{n} \sum_{i=1}^{n} I(y_i \neq \hat{y}_i)
$$


<h4>Same Equation</h4>

$$
\text{Ave} \left( I(y_0 \neq \hat{y}_0) \right)
$$


<h3> Explanation </h3>

* <h4><b style="color:blue">n:</b> The total number of observations or samples.</h4>
* <h4><b style="color:blue">$y_{i}$:</b> The true label for the ùëñ-th observation.</h4>
* <h4><b style="color:blue">$\hat{y}_i$:</b> The predicted label for the ùëñ-th observation.</h4>
* <h4><b style="color:blue">$I(y_0 \neq \hat{y}_0)$:</b> This is an indicator function that returns 1 if the true label $y_{i}$ is not equal to the predicted label $\hat{y}_i$ and 0 otherwise.</h4>


1. <h4>Indicator Function: For each observation, check if the true label and the predicted label are different. If they are different, return 1; if they are the same, return 0.</h4>

2. <h4>Summation: Sum up the results of the indicator function for all observations. This gives the total number of incorrect predictions.</h4>

3. <h4>Average: Divide the total number of incorrect predictions by the total number of observations ùëõ to get the average error rate.</h4>


<h2 style="color:blue">The Bayes Classifer</h2>

<h3 style="color:black">Bayes Classifier is a probabilistic model used in machine learning to make predictions. It is based on Bayes' theorem, which provides a way to update the probability estimate of a hypothesis given new evidence.</h3>

$$
\Pr(Y = j \mid X = x_0)
$$

<h3> Explanation </h3>

* <h4><b style="color:blue">Pr:</b> This symbol denotes probability.</h4>
* <h4><b style="color:blue">ùëå:</b> This represents the target variable or the class label.</h4>
* <h4><b style="color:blue">j:</b> This is a specific class label value that ùëå can take.</h4>
* <h4><b style="color:blue">X:</b> This represents the feature variables.</h4>
* <h4><b style="color:blue">$x_{0}$:</b> This is a specific value of the feature variables ùëã</h4>

**So, This equation means "the probability that the target variable \( Y \) equals \( j \) given that the feature variables \( X \) have the value \( $x_{0}$ \)."**


> <h3>Example</h3>

<h4>Imagine you have a model that predicts whether an email is spam (j = "spam") or not spam (j = "not spam") based on certain features of the email (like the words it contains, the sender's address, etc.).</h4>

*  <h4><b style="color:blue">X:</b> Features of the email.</h4>
*  <h4><b style="color:blue">$x_{0}$:</b> Specific features of a new email you are analyzing.</h4>
*  <h4><b style="color:blue">Y:</b> The class label (whether the email is spam or not).</h4>


$\Pr(Y = 'Spam' \mid X = x_0)$

**represents the probability that the new email (with features $x_{0}$) is spam**

> **The Bayes classifier uses this probability to decide the most likely class for the new email.**
> 
> **The classifier assigns the new email to the class with the highest probability.**
> 
> **if $\Pr(Y = 'spam' \mid X = x_0)$ is higher than  $\Pr(Y = 'not spam' \mid X = x_0)$ the email is classified as spam.**

<h2 style="color:blue">K-Nearest Neighbors (KNN) classifer.</h2>

<h4>K-Nearest Neighbors (KNN) is a simple and intuitive classification algorithm. It classifies a new data point based on the classes of its nearest neighbors in the feature space.</h4>

* <h5><b>Training Phase:</b> KNN doesn‚Äôt have a training phase in the traditional sense. It simply stores all the training data points and their corresponding class labels.</h5>

* <h5><b>Training Phase:</b> KNN doesn‚Äôt have a training phase in the traditional sense. It simply stores all the training data points and their corresponding class labels.</h5>

* <h5><b>Classification Phase:</b> To classify a new data point, the algorithm follows these steps:</h5>
  
  - <h5><b>Choose K:</b> Decide the number of neighbors (K) to consider. K is a positive integer, usually a small odd number to avoid ties.</h5>

  - <h5><b>Compute Distances:</b> Calculate the distance between the new data point and all the training data points. Common distance metrics include Euclidean distance, Manhattan distance, etc.</h5>
  
  - <h5><b>Find Nearest Neighbors:</b>Identify the K training data points that are closest to the new data point based on the calculated distances.</h5>

  - <h5><b>FindVote for Labels:</b>Look at the classes of the K nearest neighbors. The new data point is assigned to the class that appears most frequently among the K nearest neighbors (majority vote).</h5>

<h3>Example</h3>

<h4>Imagine you have a dataset of fruits with features like weight and color, and you want to classify a new fruit.</h4>

* <h5><b>1. Training Data:</b> You have data on several fruits with their weights, colors, and labels (e.g., apple, orange, banana).</h5>

* <h5><b>2. New Fruit:</b> You have a new fruit with a specific weight and color, but you don't know its label.</h5>

* <h5><b>3. Choose K:</b>Let's say you choose K = 3.</h5>

* <h5><b>4. Compute Distances:</b> Calculate the distance from the new fruit to each fruit in the training data.</h5>

* <h5><b>5. Find Nearest Neighbors:</b> Identify the 3 fruits in the training data that are closest to the new fruit.</h5>

* <h5><b>6. Vote for Labels:</b> If 2 out of the 3 nearest fruits are apples and 1 is an orange, the new fruit will be classified as an apple.</h5>


<h3>Key Points</h3>

**1. K Value: Choosing the right K value is crucial. A small K value can be noisy and lead to overfitting, while a large K value can smooth out the classification and lead to underfitting.**

**2.Distance Metric: The choice of distance metric can affect the classification results.**

**3.Simple and Intuitive: KNN is easy to understand and implement but can be computationally expensive for large datasets because it requires distance calculations for all data points.**


<h3>Summary</h3>

<h4>K-Nearest Neighbors (KNN) is a straightforward and powerful classification algorithm that assigns a class to a new data point based on the classes of its nearest neighbors in the feature space. It relies on the intuition that similar data points (neighbors) tend to belong to the same class.</h4>








<a id='2.3'></a>
<h4 style="color:red">-----------------------------------------------------------------------------------------------------------------------------</h4>
<h4 style= "float: right"> <a href="#TOC" >Go to Table of content</a> </h4>

## 2.2.3 Python<h4 style="color:red">-----------------------------------------------------------------------------------------------------------------------------</h4>




```python

```


```python

```


```python

```

<a id='LR'></a>
<div 
     style="border-width:0.05;
            border-radius: 15px;
            border-style: solid;
            border-color: lightblue;
            background-color: white;
            text-align: center;
            font: 14pt 'Candara';
            font-weight:bold;padding-bottom: 15px;
            width: 50%;
            padding:10px;
            margin:0;
            ">
    <h1
        style = " 
                 color: black;
                 text-align: center;
                 font-family: Trebuchet MS;
                 padding:10px;
                 margin:0
                 "> 3. Linear Regression
    </h1>
</div></h1>
</div>

### Linear regression analysis allows us to investigate several key relationships between variables. 
#### Here are a few important questions that we might seek to address (Examples)
* #####  1. Is there a relationship between advertising budget and sales?
* #####  2. How strong is the relationship between advertising budget and sales?
* #####  3. Which media are associated with sales?
* #####  4. How large is the association between each medium and sales?
* #####  5. How accurately can we predict future sales?
* #####  6. Is the relationship linear?
* #####  7. Is there synergy among (`interaction effect`) the advertising media?


<a id='3.1'></a>
<h4 style="color:red">-----------------------------------------------------------------------------------------------------------------------------</h4>
<h4 style= "float: right"> <a href="#TOC" >Go to Table of content</a> </h4>

## 3.1 Simple Linear Regression<h4 style="color:red">-----------------------------------------------------------------------------------------------------------------------------</h4>



#### Simple linear regression lives up to its name: it is a very straightforward simple linear approach for predicting a quantitative response Y on the basis of a sin gle predictor variable X. It assumes that there is approximately a linear relationship between X and Y. Mathematically, we can write this linear relationship as

<h2 style= "text-align:center; color:blue"> Y ‚âà $Œ≤_{0}$ + $Œ≤_{1}$X </h2>

**ùëå: This represents the outcome or the dependent variable. It's what you are trying to predict or explain.**

**$Œ≤_{0}$ : This is the intercept. It's a constant term that represents the value of ùëå when ùëã is zero. Think of it as the starting point or baseline value of ùëå**
‚Äã

**$Œ≤_{1}$ : This is the slope or the coefficient for ùëã. It tells you how much  ùëå changes when ùëã changes by one unit. It's a measure of the relationship between ùëã and ùëå.**


#### Example 
<h2 style= "text-align:center; color:blue">Sales ‚âà $Œ≤_{0}$ + $Œ≤_{1}$TV </h2>



#### Once we have used our training data to produce estimates $\hat{Œ≤_{0}}$ and $\hat{Œ≤_{1}}$ for the model coefficients, we can predict future sales on the basis of a particular value of TV advertising by computing

<h2 style= "text-align:center; color:blue"> $\hat{y} ‚âà \hat{Œ≤_{0}} + \hat{Œ≤_{1}}x$</h2>

**$\hat{ùëå}$: indicates a prediction of Y.**

<a id='3.1.1'></a>
<h4 style="color:red">-----------------------------------------------------------------------------------------------------------------------------</h4>
<h4 style= "float: right"> <a href="#TOC" >Go to Table of content</a> </h4>

## 3.1.1 Estimating the Coefficients<h4 style="color:red">-----------------------------------------------------------------------------------------------------------------------------</h4>



# Residual Sum of Squares (RSS)

RSS stands for Residual Sum of Squares. It's a measure used in statistics and data analysis to assess the goodness of fit of a regression model. Let's break it down in simple terms.

### What is RSS?

1. **Residual**: The difference between the actual value and the predicted value from a regression model.
<h6></h6>
   \[
   \text{Residual} = \text{Actual Value} - \text{Predicted Value}
   \]

3. **Sum of Squares**: Squaring the residuals ensures they are all positive (since squaring a negative number makes it positive) and gives more weight to larger differences. The sum of these squared residuals across all data points is the RSS.
   <h1></h1>
   \[
   \text{RSS} = \sum (\text{Actual Value} - \text{Predicted Value})^2
   \]

### Why is RSS Important?

- **Measure of Fit**: RSS indicates how well your regression model fits the data. A lower RSS means a better fit because the model's predictions are closer to the actual values.
- **Model Comparison**: When comparing different models, the one with the lower RSS is generally preferred, as it has less error in its predictions.

### Example

Imagine you have a simple dataset with actual values of \( Y \) and predictions from your model:

| Data Point | Actual Value (\( Y \)) | Predicted Value (\($\hat{Y}$\)) | Residual (\( Y - $\hat{Y}$ \)) | Squared Residual (\( (Y - $\hat{Y}$)^2 \)) |
|------------|------------------------|--------------------------------|-----------------------------|----------------------------------------|
| 1          | 10                     | 8                              | 2                           | 4                                      |
| 2          | 15                     | 14                             | 1                           | 1                                      |
| 3          | 14                     | 12                             | 2                           | 4                                      |
| 4          | 12                     | 10                             | 2                           | 4                                      |

To calculate the RSS:

1. Compute the residuals for each data point.
2. Square each residual.
3. Sum up all the squared residuals.

For this example:
<h2 style= "text-align:center; color:blue"> RSS = 4 + 1 + 4 + 4 = 13</h2>



### Summary

- RSS quantifies the error of a regression model.
- Lower RSS indicates a model that better fits the data.
- It is used for evaluating and comparing different regression models.
- Our Target to make RSS as small as possible


<a id='3.1.2'></a>
<h4 style="color:red">-----------------------------------------------------------------------------------------------------------------------------</h4>
<h4 style= "float: right"> <a href="#TOC" >Go to Table of content</a> </h4>

## 3.1.2 Assessing the Accuracy of the Coefficient Estimates
<h4 style="color:red">-----------------------------------------------------------------------------------------------------------------------------</h4>



<h2 style= "text-align:center; color:blue"> Y = $Œ≤_{0}$ + $Œ≤_{1}$X </h2>

### This linear regression equation assumes a perfect linear relationship without any error. 

<h2 style= "text-align:center; color:blue"> Y = $Œ≤_{0}$ + $Œ≤_{1}$X + $\epsilon$ </h2>

**$\epsilon$ : Error term (the difference between the observed value and the value predicted by the model).**

### The second linear regression equation is more realistic for most real-world applications, where data typically exhibit some level of randomness and variability that the model cannot capture perfectly


### The presence of the error term allows for statistical inference, such as hypothesis testing and confidence interval estimation for the coefficients.

<h2 style="text-align:center; color:red"> Population Mean ¬µ vs. Sample Mean $\hat{¬µ}$</h2>
<h3 style="text-align:center; color:blue"> »≥ =  $\hat{¬µ}$</h3>

### Imagine you want to know the average height (population mean, ¬µ) of all the people in a city. But you can't measure everyone's height, so you measure the heights of 100 people (your sample).

* #### Population Mean (¬µ): The true average height of everyone in the city.
* #### Sample Mean (»≥): The average height of the 100 people you measured.

### The sample mean (»≥) is a good estimate of the population mean (¬µ). While they are not the same, the sample mean gives us a reasonable idea of what the population mean might be.



<h2 style="text-align:center; color:red">  Unbiased Estimators</h2>

### An estimator is unbiased if, on average, it hits the true value. For example:

#### If we take many different samples and calculate the sample mean (»≥)  for each one, the average of all these sample means will be very close to the population mean (¬µ). This makes »≥ an unbiased estimator of ¬µ.

#### Similarly, the estimates $\hat{Œ≤_{0}}$ and $\hat{Œ≤_{1}}$ are unbiased for the true coefficients ${Œ≤_{0}}$ and ${Œ≤_{1}}$. If we took many different samples and calculated $\hat{Œ≤_{0}}$ and $\hat{Œ≤_{1}}$ each time, the average of these estimates would be very close to the true ${Œ≤_{0}}$ and ${Œ≤_{1}}$.

<h2 style="text-align:center; color:red">  Standard Error </h2>

### The standard error (SE) tells us how much the sample mean (»≥) or  is likely to vary from the population mean (¬µ). It gives us an idea of the accuracy of our estimate.

* #### If the SE is small, our sample mean is likely to be close to the population mean.
* #### If the SE is large, our sample mean might be far from the population mean.

<h2 style="text-align:center; color:red">  Variance and Standard Error </h2>

### Variance of the Estimate of the Mean ( $\hat{¬µ}$) and Its Standard Error:


<h2 style= "text-align:center; color:blue"> Var($\hat{\mu}$) = SE($\hat{\mu}$)$^2$ = $\frac{\sigma^2}{n}$</h2>

**œÉ: The standard deviation of the actual Y Values.**

**n: The number of observations (data points).**

**SE($\hat{\mu}$): The standard error of the estimated mean ($\hat{\mu}$), which tells us how much $\hat{\mu}$ is expected to vary from the actual mean (ùúá).**


<h2 style="text-align:center; color:red">  Residual Standard Error (RSE) </h2>

### When we perform linear regression, we estimate the coefficients (intercept and slope). We want to know how close these estimates $\hat{B_{0}}$ and $\hat{B_{1}}$ are to the true values ${B_{0}}$ and ${B_{1}}$. To measure this, we use the standard errors (SE) of the estimates.

### Since $\sigma^2$ or SE is often unknown, it is estimated from the data. This estimate is known as the residual standard error (RSE):


<h2 style= "text-align:center; color:blue"> RSE = $\sqrt{RSS / (n-2)}$ </h2>

<h2 style="text-align:center; color:red;">   Confidence Intervals </h2>

### Standard errors can be used to compute confidence intervals. A 95% confidence interval gives a range of values within which we expect the true parameter value to lie with 95% probability.


####  Confidence Intervals for ${B_{1}}$

<h2 style= "text-align:center; color:blue"> $\hat{B_{1}}$ ¬± 2.SE($\hat{B_{1}}$)</h2>

#### There is approximately a 95% chance that the following interval will contain the true value of ${B_{1}}$
<h2 style= "text-align:center; color:blue"> [$\hat{B_{1}}$ + 2.SE($\hat{B_{1}}$) , $\hat{B_{1}}$ - 2.SE($\hat{B_{1}}$)]</h2>

####  Confidence Intervals for ${B_{0}}$

<h2 style= "text-align:center; color:blue"> $\hat{B_{0}}$ ¬± 2.SE($\hat{B_{0}}$)</h2>

#### There is approximately a 95% chance that the following interval will contain the true value of ${B_{0}}$
<h2 style= "text-align:center; color:blue"> [$\hat{B_{0}}$ + 2.SE($\hat{B_{0}}$) , $\hat{B_{0}}$ - 2.SE($\hat{B_{0}}$)]</h2>




<h2 style= "; color:green"> In the case of the advertising data, the 95% confidence interval for $B_{0}$
 is [6.130 , 7.935] and the 95% confidence interval for $B_{1}$ is [0.042 , 0.053]</h2>
<h2 style= "; color:blue">Therefore, we can conclude that in the absence of any advertising, sales will,
 on average, fall somewhere between 6,130 and 7,935 units.(When X is 0 Intercept)</h2> 
 <h2 style= "; color:blue">Furthermore, for each $1,000 increase in television advertising, there will be an average
 increase in sales of between 42 and 53 units.</h2>

<h2 style="text-align:center; color:red"> Hypothesis Testing </h2>

### Standard errors can also be used to perform hypothesis tests on the coefficients. The most common hypothesis test involves testing the null hypothesis of

<h2 style= "text-align:center; color:blue"> $H_{0}$:  There is no relationship between X and Y</h2>

####  versus the alternative hypothesis

<h2 style= "text-align:center; color:blue"> $H_{a}$:   There is some relationship between X and Y</h2>


###  Mathematically, this corresponds to testing

<h2 style= "text-align:center; color:blue"> $H_{0}$: $B_{1}$=0 </h2>

#### versus

<h2 style= "text-align:center; color:blue"> $H_{a}$: $B_{1}$ $\neq$ 0</h2>


### if $B_{1}$ =0 then Y = $B_{1}$ + $\epsilon$ and X is not associated with Y

### To test the null hypothesis, we need to determine whether $\hat{B_{1}}$, our estimate for $B_{1}$, is sufficiently far from zero that we can be confident that $B_{1}$ is non-zero. 

### How far is far enough? 

### This of course depends on the accuracy of  $\hat{B_{1}}$  which depends on SE($\hat{B_{1}}$ ).

### if SE($\hat{B_{1}}$ ) is small, then even relatively small values of $\hat{B_{1}}$ may provide strong evidence that $\hat{B_{1}}$ $\neq$0, and hence that there is a relationship between X and Y. 

### In contrast, if SE($\hat{B_{1}}$) is large, then $\hat{B_{1}}$ must be large in absolute value in order for us to reject the null hypothesis. In practice, we compute a t-statistic, t-statistic given by

<h2 style= "text-align:center; color:blue"> t = $\frac{\hat{B_1} - 0}{SE(\hat{B_1})}$</h2>




| Predictor  | Coefficient | Std. Error | t-statistic | p-value |
|------------|-------------|------------|-------------|---------|
| Intercept  | 7.0325      | 0.4578     | 15.36       | <0.0001 |
| TV         | 0.0475      | 0.0027     | 17.67       | <0.0001 |


### From above Results We can see that


#### 1. Intercept
* **Coefficient ( $B_{0}$) `7.0325`: This means that the expected value of the dependent variable (Sales) is 7.0325 when all predictor variables are zero. (When TV Advertising = 0)**

* **Std. Error `0.4578`: This is the standard error of the intercept term. (smaller standard error indicates more precise estimates of the coefficient)**

* **t-statistic `15.36`: This is the t-value from t-statistics test (It is used to test whether a coefficient is significantly different from zero to reject the null hypothesis)**

* **p-value `<0.0001`: This very small p-value indicates that the intercept term is highly significant.**


#### 2. TV
* **Coefficient ( $B_{1}$ ) `0.0475`: This means that for each additional unit of TV advertising, the Sales is expected to increase by 0.0475 units, holding other variables constant.**

* **Std. Error `0.0027`: This is the standard error of the intercept term. (smaller standard error indicates more precise estimates of the coefficient)**

* **t-statistic `17.67`: This is the t-value from t-statistics test (It is used to test whether a coefficient is significantly different from zero to reject the null hypothesis)**

* **p-value `<0.0001`: This very small p-value indicates that the intercept term is highly significant.**

> <h3 style= "; color:blue">  t-statistics is large and p-value is Small which sgguest that there is a correlation between TV advertising and sales</h3>


<h2 style= "; color:green"> Notice that the coefficients for $\hat{B_{0}}$ and $\hat{B_{0}}$ are very large relative to their standard errors, so the t-statistics are also large; the probabilities of seeing such values if $H_{0}$ is true are virtually zero. Hence we can conclude  $B_{0}$ $\neq$ 0 and $B_{0}$ $\neq$ 0</h2>


<a id='3.1.3'></a>
<h4 style="color:red">-----------------------------------------------------------------------------------------------------------------------------</h4>
<h4 style= "float: right"> <a href="#TOC" >Go to Table of content</a> </h4>

##  3.1.3 Assessing the Accuracy of the Model<h4 style="color:red">-----------------------------------------------------------------------------------------------------------------------------</h4>



### Once we have rejected the null hypothesis $H_{0}$ in favor of the alternative hypothesis $H_{a}$, it is natural to want to quantify the extent to which the model fits the data. The quality of a linear regression fit is typically assessed using two related quantities: the residual standard error `RSE` and the `R2` statistic 

| Quantity  | Value |
|------------|-------------|
| Residual standard error  | 3.26      |
| R$^2$        | 0.612      |
|  F-statistic | 312.1    |

### From above Results We can see that

<h3 style= "; color:green"> RSE is 3.26. This means that on average, the observed values deviate from the predicted values by about 3.26 units. A smaller RSE indicates a better fit of the model to the data.</h3>

<h3 style= "; color:green"> R¬≤ of 0.612 means that 61.2% of the variability in the dependent variable can be explained by the independent variables in the model. The closer the R¬≤ is to 1, the better the model explains the variability of the dependent variable. In this context, an R¬≤ of 0.612 indicates a moderately strong relationship between the predictors and the response variable.</h3>

<h3 style= "; color:green"> F-statistic of 312.1 is quite high, indicating that the model is statistically significant. A high F-statistic suggests that the model provides a better fit to the data than a model with no predictors. The corresponding p-value would confirm the overall significance of the model..</h3>







* ##### Residual Standard Error (RSE):  is a measure of the quality of a linear regression fit. It represents the average amount by which the observed values differ from the values predicted by the model.

* ##### R-squared( R$^2$ ) :  is the proportion of the variance in the dependent variable that is predictable from the independent variables. It ranges from 0 to 1.

* ##### F-Statistic: measures the overall significance of the regression model. It tests whether at least one of the predictor variables has a non-zero coefficient.


<a id='3.2'></a>
<h4 style="color:red">-----------------------------------------------------------------------------------------------------------------------------</h4>
<h4 style= "float: right"> <a href="#TOC" >Go to Table of content</a> </h4>

##   3.2 Multiple Linear Regression<h4 style="color:red">-----------------------------------------------------------------------------------------------------------------------------</h4>



<h2 style= "; color:green">  Simple linear regression is a useful approach for predicting a response on the basis of a single predictor variable. However, in practice we often have more than one predictor. However, the approach of fitting a separate simple linear regression model
for each predictor is not entirely satisfactory.</h2>

<h2 style= "; color:green">  Instead of fitting a separate simple linear regression model for each predictor, a better approach is to extend the simple linear regression model so  it can directly accommodate multiple predictors. We can do this by giving each predictor a separate slope coefficient in a single model. In general, suppose that we have p distinct predictors. Then the multiple linear regression model takes the form </h2>


<h2 style= "text-align:center; color:blue"> Y =$B_{0}$ + $B_{1}$ $X_{1}$ + $B_{2}$ $X_{2}$ + ..... + + $B_{p}$ $X_{p}$ + $\epsilon$</h2>


#### for Example:

<h2 style= "text-align:center; color:blue">  sales = $B_{0}$  + $B_{1}$ * TV + $B_{2}$ * radio + $B_{3}$ * newspaper +   $\epsilon$.


<a id='3.2.1'></a>
<h4 style="color:red">-----------------------------------------------------------------------------------------------------------------------------</h4>
<h4 style= "float: right"> <a href="#TOC" >Go to Table of content</a> </h4>

##    3.2.1 Estimating the Regression Coefficients
<h4 style="color:red">-----------------------------------------------------------------------------------------------------------------------------</h4>



#### Multiple Linear Regression

| Predictor   | Coefficient | Std. Error | t-statistic | p-value  |
|-------------|-------------|------------|-------------|----------|
| Intercept   | 2.939       | 0.3119     | 9.42        | <0.0001  |
| TV          | 0.046       | 0.0014     | 32.81       | <0.0001  |
| radio       | 0.189       | 0.0086     | 21.89       | <0.0001  |
| newspaper   | 0.001       | 0.0059     | 0.18        | 0.8599   |




### From above Results We can see that


#### 1. Intercept
* **Coefficient ( $B_{0}$) `2.939`: This means that the expected value of the dependent variable (Sales) is 2.939 when all predictor variables are zero. (TV Advertising, Radio Advertising and Newspaper Advertising = 0)**

* **Std. Error `0.3119 `: This is the standard error of the intercept term. (smaller standard error indicates more precise estimates of the coefficient)**

* **t-statistic `9.42`: This is the t-value from t-statistics test (It is used to test whether a coefficient is significantly different from zero to reject the null hypothesis)**

* **p-value `<0.0001`: This very small p-value indicates that the intercept term is highly significant.**


#### 2. TV
* **Coefficient ( $B_{1}$ ) `0.046`: This means that for each additional unit of TV advertising, the Sales is expected to increase by 0.046 units, holding other variables constant.**

* **Std. Error `0.0014`: This is the standard error of the intercept term. (smaller standard error indicates more precise estimates of the coefficient)**

* **t-statistic ` 32.81`: This is the t-value from t-statistics test (It is used to test whether a coefficient is significantly different from zero to reject the null hypothesis)**

* **p-value `<0.0001`: This very small p-value indicates that the intercept term is highly significant.**

> <h3 style= "; color:blue">  t-statistics is large and p-value is Small which sgguest that there is a correlation between TV advertising and sales</h3>

#### 3. Radio
* **Coefficient ( $B_{1}$ ) `0.189`: This means that for each additional unit of Radio advertising, the Sales is expected to increase by 0.189 units, holding other variables constant.**

* **Std. Error `0.0086`: This is the standard error of the intercept term. (smaller standard error indicates more precise estimates of the coefficient)**

* **statistic `21.89`: This is the t-value from t-statistics test (It is used to test whether a coefficient is significantly different from zero to reject the null hypothesis)**

* **p-value `<0.0001`: This very small p-value indicates that the intercept term is highly significant.**

> <h3 style= "; color:blue">  t-statistics is large and p-value is Small which sgguest that there is a correlation between Radio advertising and sales</h3>



#### 4. Newspaper
* **Coefficient ( $B_{1}$ ) `0.001`: This means that for each additional unit of Newspaper advertising, the Sales is expected to increase by 0.001 units, holding other variables constant.**

* **Std. Error `0.0059`: This is the standard error of the intercept term. (smaller standard error indicates more precise estimates of the coefficient)**

* **t-statistic `0.18`: This is the t-value from t-statistics test (It is used to test whether a coefficient is significantly different from zero to reject the null hypothesis)**

* **p-value `<0.0001`: This very small p-value indicates that the intercept term is highly significant.**

> <h3 style= "; color:blue">   t-statistics is small which sgguest that there is no correlation between Newspaper advertising and sales</h3>



#### Correlations


|         | TV      | radio   | newspaper | sales   |
|---------|---------|---------|-----------|---------|
| TV      | 1.0000  | 0.0548  | 0.0567    | 0.7822  |
| radio   |         | 1.0000  |  0.3541   | 0.5762|
| newspaper|   |    | 1.0000    | 0.2283        |
| sales   |   |         |           | 1.0000  |




<a id='3.2.2'></a>
<h4 style="color:red">-----------------------------------------------------------------------------------------------------------------------------</h4>
<h4 style= "float: right"> <a href="#TOC" >Go to Table of content</a> </h4>

##     3.2.2 Some Important Questions
<h4 style="color:red">-----------------------------------------------------------------------------------------------------------------------------</h4>



##  1. Is at least one of the predictors X1,X2,...,Xp useful in predicting the response?

#### Recall that in the simple linear regression setting, in order to determine whether there is a relationship between the response and the predictor we can simply check whether $B_{1}$ =0. In the multiple regression setting with p ppredictors, we need to ask whether all of the regression coefficients are zero, whether $B_{1}$ = $B_{2}$ =  ¬∑¬∑¬∑= $B_{p}$ = 0 .As in the simple linear regression setting, we use a hypothesis test to answer this question. We test the null hypothesis,

* #### Null hypothesis

<h2 style= "text-align:center; color:blue"> $H_{0}$: $B_{1}$=$B_{2}$= ..... =$B_{p}$ = 0 </h2>

* #### versus Alternative hypothesis

<h2 style= "text-align:center; color:blue"> $H_{a}$: at least one $B_{j}$ is non-zero </h2>


<h2 style= "; color:red"> Hence, when there is no relationship between the response and predictors, one would expect the F-statistic to take on a value close to 1.  On the otherhand, if $H_{a}$ is true, then we expect F to be greater than 1.</h2>

| Quantity  | Value |
|------------|-------------|
| Residual standard error  | 1.69      |
| R$^2$        | 0.897      |
|  F-statistic | 570   |

### From above Results We can see that

<h3 style= "; color:green"> RSE is 1.69. This means that on average, the observed values deviate from the predicted values by about 1.69 units. A smaller RSE indicates a better fit of the model to the data.</h3>

<h3 style= "; color:green"> R¬≤ of 0.897 means that 61.2% of the variability in the dependent variable can be explained by the independent variables in the model. The closer the R¬≤ is to 1, the better the model explains the variability of the dependent variable. In this context, an R¬≤ of 0.897 indicates a moderately strong relationship between the predictors and the response variable.</h3>

<h3 style= "; color:green"> F-statistic of 570 is quite high, indicating that the model is statistically significant. A high F-statistic suggests that the model provides a better fit to the data than a model with no predictors. The corresponding p-value would confirm the overall significance of the model..</h3>







<a id='3.3'></a>
<h4 style="color:red">-----------------------------------------------------------------------------------------------------------------------------</h4>
<h4 style= "float: right"> <a href="#TOC" >Go to Table of content</a> </h4>

##      3.3 Other Considerations in the Regression Model

<h4 style="color:red">-----------------------------------------------------------------------------------------------------------------------------</h4>



<a id='3.3.1'></a>
<h4 style="color:red">-----------------------------------------------------------------------------------------------------------------------------</h4>
<h4 style= "float: right"> <a href="#TOC" >Go to Table of content</a> </h4>

##      3.3.1 Qualitative Predictors.

<h4 style="color:red">-----------------------------------------------------------------------------------------------------------------------------</h4>



###  In our discussion so far, we have assumed that all variables in our linear regression model are quantitative. But in practice, this is not necessarily the case; often some predictors are qualitative.

### Predictors with Only Two Levels

#### Suppose that we wish to investigate differences in credit card balance between those who own a house and those who don‚Äôt, ignoring the other variables for the moment. If a qualitative predictor (also known as a factor) only has two levels, or possible values, then incorporating it into a regression model is very simple. We simply create an indicator or dummy variable that takes on two possible numerical valuesÿ≤ For example, based on the own variable, we can create a new variable that takes the form



[![1.jpg](https://i.postimg.cc/VkCmkd5g/1.jpg)](https://postimg.cc/PCTB6fWv)

### Now $B_{0}$ can be interpreted as the average credit card balance among those who do not own, $B_{0}$ + $B_{1}$ as the average credit card balance among those who do own their house, and $B_{1}$ as the average difference in credit card balance between owners and non-owners.

| Predictor  | Coefficient | Std. Error | t-statistic | p-value |
|------------|-------------|------------|-------------|---------|
| Intercept  | 509.80      | 33.13     | 15.389       | <0.0001 |
| Own[Yes]         | 19.73      | 46.05     | 0.429       | 0.6690 |


### The average credit card debt for non-owners is estimated to be $509.80, whereas owners are estimated to carry $19.73 in additional debt for a total of $509.80 + $19.73 = $529.53. However, we notice that the p-value for the dummy variable is very high. This indicates that there is no statistical evidence of a difference in average credit card balance based on house ownership

### Now $B_{0}$ can be interpreted as the overall average credit card balance (ignoring the house ownership effect), and $B_{1}$ is the amount by which houseowners and non-owners have credit card balances that are above and below the average, respectively. In this example, the estimate for $B_{0}$ is  519.665 dollar, halfway between the non-owner and owner averages of 509.80 dollar and  529.53. The estimate for $B_{1}$ is  9.865 dollar, which is half of  19.73 dollar, the average difference between owners and non-owners. It is important to note that the final pre dictions for the credit balances of owners and non-owners will be identical regardless of the coding scheme used. The only difference is in the way that the coefficients are interpreted

###  Qualitative Predictors with More than Two Levels



### When a qualitative predictor has more than two levels, a single dummy variable cannot represent all possible values. In this situation, we can create additional dummy variables. For example, for the region variable we create two dummy variables. The first could be

[![2.jpg](https://i.postimg.cc/KcTYYqVB/2.jpg)](https://postimg.cc/Sjy4Trmx)

### Now 0canbeinterpreted as the average credit card balance for individuals from the East, $B_{1}$ can be interpreted as the difference in the average balance between people from the South versus the East, and $B_{2}$ can be interpreted as the difference in the average balance between those from the West versus the East. There will always be one fewer dummy variable than the number of levels. The level with no dummy variable‚ÄîEast in this example‚Äîis known as the baseline

| Predictor  | Coefficient | Std. Error | t-statistic | p-value |
|------------|-------------|------------|-------------|---------|
| Intercept  | 531.00      | 46.32     | 11.464       | <0.0001 |
| region[South]         | -12.50      | 56.68     | -0.221       | 0.8260 |
| region[West]         | -18.69      | 65.02     | -0.287       | 0.7740 |


**1. Intercept (531.00): This is the baseline value when the region is not South or West (likely representing a reference category like "North" or "East"). The value 531.00 is the average prediction when the other variables are zero.**

**2.region[South] (-12.50): This value tells us that being in the South decreases the predicted value by 12.50 units compared to the reference category (e.g., North or East). However, the large p-value (0.8260) indicates that this effect is not statistically significant, meaning that we don't have enough evidence to say that the South region has a real impact on the outcome.**

**3. region[West] (-18.69): This suggests that being in the West decreases the predicted value by 18.69 units compared to the reference category. Similarly, the p-value (0.7740) is large, meaning this effect is also not statistically significant.**

**4. Summary: The intercept is significant, meaning the base value (for the reference region) is meaningful. However, the effects of the South and West regions on the outcome are not statistically significant, meaning there's no strong evidence that these regions have a real impact on the outcome in this model.**

<h2 style= "text-align:left; color:black">we see that the estimated balance for the baseline, East, is 531 dollar It is estimated that those in the South will have 18.69 less debt than those in the East, and that those in the West will have $12.50 less debt than those in the East. However, the p-values associated with the coefficient estimates for the two dummy variables are very large, suggesting no statistical evidence of a real difference in average credit card balance between South and East or between West and East.
</h2>

<h2 style= "text-align:left; color:black">Once again, the
 level selected as the baseline category is arbitrary, and the final predictions
 for each group will be the same regardless of this choice. However, the
 coefficients and their p-values do depend on the choice of dummy variable
 coding.</h2>


 <h2 style= "text-align:left; color:black">Using this dummy variable approach presents no difficulties when in
corporating both quantitative and qualitative predictors. For example, to
 regress balance on both a quantitative variable such as income and a qual
itative variable such as student, we must simply create a dummy variable
 for student and then fit a multiple regression model using income and the
 dummy variable as predictors for credit card balance.</h2>

 

<a id='3.3.2'></a>
<h4 style="color:red">-----------------------------------------------------------------------------------------------------------------------------</h4>
<h4 style= "float: right"> <a href="#TOC" >Go to Table of content</a> </h4>

##      3.3.2  Extensions of the Linear Model.

<h4 style="color:red">-----------------------------------------------------------------------------------------------------------------------------</h4>



### Two of the most important assumptions state that the relationship between the predictors and response are additive and linear. The additivity assumption means that the association between a predictor Xj and the response Y does not depend on the values of the other predictors. The linearity assumption states that the change in the response Y associated with a one-unit change in Xj is constant, regardless of the value of Xj. In later chapters of this book, we examine a number of sophisticated methods that relax these two assumptions. Here, we briefly examine some common classical approaches  for extending the linear mode

 

##  Removing the Additive Assumption


### In our previous analysis of the Advertising data, we concluded that both TVand radio seem to be associated with sales. The linear models that formed the basis for this conclusion assumed that the effect on sales of increasing one advertising medium is independent of the amount spent on the other media. For example, the linear model states that the average increase in sales associated with a one-unit increase in TV is always 1, regardless of the amount spent on radio.

###  However, this simple model may be incorrect. Suppose that spending money on radio advertising actually increases the effectiveness of TV advertising, so that the slope term for TV should increase as radio increases.

###  In this situation, given a fixed budget of $100,000, spending half on radio and half on TV may increase sales more than allocating the entire amount to either TV or to radio. In marketing, this is known as a synergy effect, and in statistics it is referred to as an interaction effect. 

<h2 style= "text-align:center; color:blue"> Y =$B_{0}$ + $B_{1}$ $X_{1}$ + $B_{2}$ $X_{2}$ + $\epsilon$</h2>


### According to this model, a one-unit increase in X1 is associated with an average increase in Y of $B_{1}$ units. Notice that the presence of X2 doesnot alter this statement‚Äîthat is, regardless of the value of X2, a one unit increase in X1 is associated with a $B_{1}$-unit increase in Y . One way of extending this model is to include a third predictor, called an interaction term, which is constructed by computing the product of X1 and X2. This results in the model.


[![4.jpg](https://i.postimg.cc/3w1dyf9r/4.jpg)](https://postimg.cc/xkk0hRWB)

<h2 style= "text-align:center; color:blue"> Y =$B_{0}$ + $B_{1}$ $X_{1}$ + $B_{2}$ $X_{2}$ + $B_{3}$ $X_{1}$ $X_{2}$ +$\epsilon$</h2>


### For example, suppose that we are interested in studying the productivity of a factory. We wish to predict the number of units produced on the basis of the number of production lines and the total number of workers. It seems likely that the effect of increasing the number of production lines will depend on the number of workers, since if no workers are available to operate the lines, then increasing the number of lines will not increase production. This suggests that it would be appropriate to include an inter action term between lines and workers in a linear model to predict units. Suppose that when we fit the model

## Non-linear Relationships


[![5.jpg](https://i.postimg.cc/Y051XD5K/5.jpg)](https://postimg.cc/0KCMjn5c)

### Theapproach that we have just described for extending the linear model to accommodate non-linear relationships is known as `polynomial regresssion` , since we have included polynomial functions of the predictors in the regression model

<a id='3.3.3'></a>
<h4 style="color:red">-----------------------------------------------------------------------------------------------------------------------------</h4>
<h4 style= "float: right"> <a href="#TOC" >Go to Table of content</a> </h4>

##      3.3.3 Potential Problems.

<h4 style="color:red">-----------------------------------------------------------------------------------------------------------------------------</h4>



### When we fit a linear regression model to a particular dataset,many problems may occur.Most common among these are the following:
 
 **1.Non-linearity of theresponse-predictor relationships.**
 
 **2. Correlation of error terms.**
 
 **3.Non-constant variance of error terms.**
 
 **4.Outliers.**
 
 **5.High-leveragepoints.**
 
 **6. Collinearity.**

## 1.Non-linearity of theresponse-predictor relationships.

### The linear regression model assumes that there is a straight-line relationship between the predictors and the response. If the true relationship is far from linear, then virtually all of the conclusions that we draw from the fit are suspect. In addition, the prediction accuracy of the model can be significantly reduced.

* ### Residual plots are a useful graphical tool for identifying non-linearity.


* ###  In the case of a multiple regression model, since there are multiple predictors, we instead plot the residuals versus the predicted (or fitted) values


### If the residual plot indicates that there are non-linear associations in the data, then a simple approach is to use non-linear transformations of the predictors, such as logX, $\sqrt{X}$, and X^2, in the regression model. In the later chapters of this book, we will discuss other more advanced non-linear approaches for addressing this issue.

## 6.Collinearity

### Collinearity refers to the situation in which two or more predictor variables are closely related to one another.

### The power of the hypothesis test‚Äîthe probability of correctly detecting a non-zero coefficient‚Äîis reduced by collinearity.

### A simple way to detect collinearity is to look at the correlation matrix of the predictors. An element of this matrix that is large in absolute value indicates a pair of highly correlated variables, and therefore a collinearity problem in the data. 

### Unfortunately, not all collinearity problems can be detected by inspection of the correlation matrix: it is possible for collinearity to exist between three or more variables even if no pair of variables has a particularly high correlation. We call this situation `multicollinearity`

### Instead of inspecting the correlation matrix, a better way to assess multi- collinearity collinearity is to compute the variance inflation factor (VIF)

### The VIF is the ratio of the variance of $\hat{B}_{j}$ when fitting the full model divided by the variance of $\hat{B}_{j}$ if fit on its own.

### The smallest possible value for VIF is 1, which indicates the complete absence of collinearity. Typically in practice there is a small amount of collinearity among the predictors. As a rule of thumb, a VIF value that exceeds 5 or 10 indicates a problematic amount of collinearity. The VIF for each variable can be computed using the formula

[![6.jpg](https://i.postimg.cc/CLS4bQVy/6.jpg)](https://postimg.cc/yWp9Hndn)

<a id='3.3.3'></a>
<h4 style="color:red">-----------------------------------------------------------------------------------------------------------------------------</h4>
<h4 style= "float: right"> <a href="#TOC" >Go to Table of content</a> </h4>

##      3.4 The Marketing Plan.

<h4 style="color:red">-----------------------------------------------------------------------------------------------------------------------------</h4>



## 1. Is there a relationship between sales and advertising budget?


### This question can be answered by fitting a multiple regression model of sales onto TV, radio, and newspaper

<h2 style= "text-align:center; color:blue"> $H_{0}$ =$B_{TV}$ = $B_{radio}$ = $B_{newspaper}$ = 0</h2>

### F-statistic can be used to determine whether or not we should reject this null hypothesis. In this case the p-value corresponding to the F-statistic is very low, indicating clear evidence of a relationship between advertising and sales.

## 2. How strong is the relationship?


### We discussed two measures of model accuracy:

* #### 1. the RSE estimates the standard deviation of the response from the population regression line. For the Advertising data, the RSE is 1.69 units while the mean value for the response is 14.022, indicating a percentage error of roughly 12 %.

* #### 2. the R<sup>2</sup> statistic records the percentage of variability in the response that is explained by the predictors. The predictors explain almost 90 % of the variance in sales.

## 3. Which media are associated with sales?


### To answer this question, we can examine the p-values associated with each predictor‚Äôs t-statistic

### In the multiple linear regression, the p-values for TV and radio are low, but the p-value for newspaper is not. This suggests that only TV and radio are related to sales.

## 4.How large is the association between each medium and sales?


### For the Advertising data, we can use the results in Table 3.4 to compute the 95 % confidence intervals for the coefficients in a multiple regression model using all three media budgets as predictors. The confidence intervals are as follows:
<h4 style= "text-align:center; color:blue">TV (0.043, 0.049)</h4>
<h4 style= "text-align:center; color:blue">Radio (0.172, 0.206)</h4>
<h4 style= "text-align:center; color:blue">newspaper.(‚àí0.013, 0.011)</h4>

### The confidence intervals for TV and radio are narrow and far from zero, providing evidence that these media are related to sales. But the interval for newspaper includes zero, indicating that the variable is not statistically significant given the values of TV and radio.

## 5.How accurately can we predict future sales?

### The accuracy associated with this estimate depends on whether we wish to predict an individual response, 
<h4 style= "text-align:center; color:blue"> Y = f(X) + $\epsilon$</h4>


## 6. Is the relationship linear?

### residual plots can be used in order to identify non-linearity. If the relationships are linear, then the residual plots should display no pattern. In the case of the Advertising data, we observe a non-linear effect

### discussed the inclusion of transformations of the predictors in the linear regression model in order to accommodate non-linear relationships.

## 7. Is there synergy among the advertising media?

### The standard linear regression model assumes an additive relationship between the predictors and the response. An additive model is easy to interpret because the association between each predictor and the response is unrelated to the values of the other predictors. However, the additive assumption may be unrealistic for certain data sets.

### in the regression model in order to accommodate non-additive relationships. A small p-value associated with the interaction term indicates the presence of such relationships

### the Advertising data may not be additive. Including an interaction term in the model results in a substantial increase in R2, from around 90 % to almost 97 %.

<a id='CL'></a>
<div 
     style="border-width:0.05;
            border-radius: 15px;
            border-style: solid;
            border-color: lightblue;
            background-color: white;
            text-align: center;
            font: 14pt 'Candara';
            font-weight:bold;padding-bottom: 15px;
            width: 50%;
            padding:10px;
            margin:0;
            ">
    <h1
        style = " 
                 color: black;
                 text-align: center;
                 font-family: Trebuchet MS;
                 padding:10px;
                 margin:0
                 "> 4. Classification
    </h1>
</div></h1>
</div>

<a id='4.1'></a>
<h4 style="color:red">-----------------------------------------------------------------------------------------------------------------------------</h4>
<h4 style= "float: right"> <a href="#TOC" >Go to Table of content</a> </h4>

##      4.1 An Overview of Classification.

<h4 style="color:red">-----------------------------------------------------------------------------------------------------------------------------</h4>



###  Classification problems occur often, perhaps even more so than regression problems for Example:

> 1. ####  A person arrives at the emergency room with a set of symptoms that could possibly be attributed to one of three medical conditions. Which of the three conditions does the individual have?

> 2. ####  An online banking service must be able to determine whether or not  a transaction being performed on the site is fraudulent, on the basi  of the user‚Äôs IP address, past transaction history, and so forth.

> 3. #### On the basis of DNA sequence data for a number of patients with  and without a given disease, a biologist would like to figure out whic  DNA mutations are deleterious (disease-causing) and which are n

<a id='4.2'></a>
<h4 style="color:red">-----------------------------------------------------------------------------------------------------------------------------</h4>
<h4 style= "float: right"> <a href="#TOC" >Go to Table of content</a> </h4>

##       4.2 Why Not Linear Regression?

<h4 style="color:red">-----------------------------------------------------------------------------------------------------------------------------</h4>



<h3>We have stated that linear regression is not appropriate in the case of a
 qualitative response. Why not?
 Suppose that we are trying to predict the medical condition of a patient
 in the emergency room on the basis of her symptoms. In this simplified
 example, there are three possible diagnoses: stroke, drug overdose, and epileptic seizure.
We could consider encoding these values as aquantitative response variable,Y  as follows:
</h3>

[![1.jpg](https://i.postimg.cc/QC1tDH59/1.jpg)](https://postimg.cc/PvfhzfST)

<h3>Using this coding, least squares could be used to ft a linear regression model
to predict Y on the basis of a set of predictors X1,...,Xp. Unfortunately,
this coding implies an ordering on the outcomes, putting drug overdose in
between stroke and epileptic seizure, and insisting that the diference
between stroke and drug overdose is the same as the diference between
drug overdose and epileptic seizure. In practice there is no particular
reason that this needs to be the case. For instance, one could choose an
equally reasonable coding</h3>

[![2.jpg](https://i.postimg.cc/Hx7W0V6Z/2.jpg)](https://postimg.cc/jwtrtd0P)

<h3>which would imply a totally diferent relationship among the three conditions. Each of these codings would produce fundamentally diferent linear
models that would ultimately lead to diferent sets of predictions on test
observations.</h3>

<h3>If the response variable‚Äôs values did take on a natural ordering, such as
mild, moderate, and severe, and we felt the gap between mild and moderate
was similar to the gap between moderate and severe, then a 1, 2, 3 coding
would be reasonable. Unfortunately, in general there is no natural way convert a qualitative response variable with more than two levels into a
quantitative response that is ready for linear regression to</h3>

<a id='4.3'></a>
<h4 style="color:red">-----------------------------------------------------------------------------------------------------------------------------</h4>
<h4 style= "float: right"> <a href="#TOC" >Go to Table of content</a> </h4>

##       4.3 Logistic Regression

<h4 style="color:red">-----------------------------------------------------------------------------------------------------------------------------</h4>



<h3>Consider again the Default data set, where the response default falls into
one of two categories, Yes or No. Rather than modeling this response Y
directly, logistic regression models the probability that Y belongs to a particular category.
</h3>

<h3>For the Default data, logistic regression models the probability of default.
For example, the probability of default given balance can be written as</h3>

<h4 style= "text-align:center; color:blue">Pr(default = Yes|balance).</h4>


<h3>For example, one might predict default = Yes</h3>
<h3>for any individual for whom p(balance) > 0.5. Alternatively, if a company
wishes to be conservative in predicting individuals who are at risk for default, then they may choose to use a lower threshold, such as p(balance) >
0.1.</h3>

<a id='4.3.1'></a>
<h4 style="color:red">-----------------------------------------------------------------------------------------------------------------------------</h4>
<h4 style= "float: right"> <a href="#TOC" >Go to Table of content</a> </h4>

##       4.3.1 The Logistic Model

<h4 style="color:red">-----------------------------------------------------------------------------------------------------------------------------</h4>



<h3>In logistic regression, we use the logistic function,</h3>

[![3.jpg](https://i.postimg.cc/yxMZhMh9/3.jpg)](https://postimg.cc/H8tndPhs)

[![4.jpg](https://i.postimg.cc/KjMKY9XY/4.jpg)](https://postimg.cc/WqspWw1R)

<a id='4.3.2'></a>
<h4 style="color:red">-----------------------------------------------------------------------------------------------------------------------------</h4>
<h4 style= "float: right"> <a href="#TOC" >Go to Table of content</a> </h4>

##       4.3.2 Estimating the Regression Coefcients

<h4 style="color:red">-----------------------------------------------------------------------------------------------------------------------------</h4>



[![5.jpg](https://i.postimg.cc/02rPRtRg/5.jpg)](https://postimg.cc/nC86qGhT)

[![6.jpg](https://i.postimg.cc/K8Mb4R8J/6.jpg)](https://postimg.cc/jDtmFqN7)

<a id='4.3.4'></a>
<h4 style="color:red">-----------------------------------------------------------------------------------------------------------------------------</h4>
<h4 style= "float: right"> <a href="#TOC" >Go to Table of content</a> </h4>

##       4.3.4 Multiple Logistic Regression

<h4 style="color:red">-----------------------------------------------------------------------------------------------------------------------------</h4>



<h3>We now consider the problem of predicting a binary response using multiple
predictors. By analogy with the extension from simple to multiple linear
regression in Chapter 3, we can generalize (4.4) as follows:</h3>

[![7.jpg](https://i.postimg.cc/hG4qsM03/7.jpg)](https://postimg.cc/hXYwPL69)

### Multiple Logistic Regression model results inerpretation

[![8.jpg](https://i.postimg.cc/FsX8GDrg/8.jpg)](https://postimg.cc/V54Kk9bd)

#### 1. Balance:
**For each one-unit increase in the balance, the log-odds of the dependent variable being 1 increases by 0.0057. Since the p-value is less than 0.0001, this variable is statistically significant, indicating that balance has a significant impact on the outcome**

#### 2. Income:
**For each one-unit increase in income, the log-odds of the dependent variable being 1 increases by 
0.0030. However, the high p-value (0.7115) suggests that income is not statistically significant in predicting the outcome, meaning its effect could be due to chance.**

#### 3. Student [Yes]:
**Being a student (Yes) decreases the log-odds of the dependent variable being 1 by 
0.6468. The p-value (0.0062) indicates that this variable is statistically significant, meaning student status has a significant effect on the outcome.**

### Summary
**(The balance and student status variables are significant predictors of the outcome.)**

**(Income does not appear to be a significant predictor in this model.)**

**(The signs of the coefficients indicate the direction of the relationship with the dependent variable: positive for balance and negative for student status.)**

<a id='4.3.5'></a>
<h4 style="color:red">-----------------------------------------------------------------------------------------------------------------------------</h4>
<h4 style= "float: right"> <a href="#TOC" >Go to Table of content</a> </h4>

##       4.3.5 Multinomial Logistic Regression

<h4 style="color:red">-----------------------------------------------------------------------------------------------------------------------------</h4>



<h3>We sometimes wish to classify a response variable that has more than two
classes. For example, we had three categories of medical condition in the emergency room: stroke, drug overdose, epileptic seizure.</h3>

<h3>However, the logistic regression approach that we have seen in this section
only allows for K = 2 classes for the response variable.</h3>

<h3>It turns out that it is possible to extend the two-class logistic regression
approach to the setting of K > 2 classes. This extension is sometimes
known as multinomial logistic regression. To do this, we frst select a single class to serve as the baseline; without loss of generality, we select the Kth class for this role. Then we replace the model with the following model</h3>

[![9.jpg](https://i.postimg.cc/qMtchTV1/9.jpg)](https://postimg.cc/rKkR37p4)

[![10.jpg](https://i.postimg.cc/bwxtkY69/10.jpg)](https://postimg.cc/v4mTdyw4)

[![11.jpg](https://i.postimg.cc/N0b2CJvy/11.jpg)](https://postimg.cc/p9hdpBTx)

<a id='4.4'></a>
<h4 style="color:red">-----------------------------------------------------------------------------------------------------------------------------</h4>
<h4 style= "float: right"> <a href="#TOC" >Go to Table of content</a> </h4>

##       4.4 Generative Models for Classifcation

<h4 style="color:red">-----------------------------------------------------------------------------------------------------------------------------</h4>



<h3>Logistic regression involves directly modeling Pr(Y = k|X = x) using the
logistic function, . In
statistical jargon, we model the conditional distribution of the response Y ,
given the predictor(s) X. We now consider an alternative and less direct
approach to estimating these probabilities. </h3>

<h3>In this new approach, we model
the distribution of the predictors X separately in each of the response
classes (i.e. for each value of Y ). We then use Bayes‚Äô theorem to fip these
around into estimates for Pr(Y = k|X = x). When the distribution of X
within each class is assumed to be normal, it turns out that the model is
very similar in form to logistic regression</h3>

## Why do we need another method, when we have logistic regression?
### There are several reasons:

*  <h3>1. When there is substantial separation between the two classes, the
parameter estimates for the logistic regression model are surprisingly
unstable. The methods that we consider in this section do not sufer
from this problem.
</h3>


*  <h3>2. If the distribution of the predictors X is approximately normal in
each of the classes and the sample size is small, then the approaches
in this section may be more accurate than logistic regression.
</h3>



*  <h3>3. The methods in this section can be naturally extended to the case
of more than two response classes. (In the case of more than two
response classes, we can also use multinomial logistic regression from
Section 4.3.5.)

</h3>

<h3>Suppose that we wish to classify an observation into one of K classes,
where K ‚â• 2. In other words, the qualitative response variable Y can take
on K possible distinct and unordered values. Let œÄk represent the overall
or prior probability that a randomly chosen observation comes from the kth class. Let fk(X) ‚â° Pr(X|Y = k)1 denote the density function of X for an observation that comes from the kth class. In other words, fk(x) is relatively large if there is a high probability that an observation in the kth
    class has X ‚âà x, and fk(x) is small if it is very unlikely that an observation in the kth class has X ‚âà x. Then Bayes‚Äô theorem states that
    In accordance with our earlier notation, we will use the abbreviation pk(x) =
Pr(Y = k|X = x); this is the posterior probability that an observation X = x belongs to the kth class. That is, it is the probability that the
observation belongs to the kth class, given the predictor value for that
observation
    
</h3>

[![12.jpg](https://i.postimg.cc/L596GfLx/12.jpg)](https://postimg.cc/Q9z3TF67)

<a id='4.4.1'></a>
<h4 style="color:red">-----------------------------------------------------------------------------------------------------------------------------</h4>
<h4 style= "float: right"> <a href="#TOC" >Go to Table of content</a> </h4>

##       4.4.1 Linear Discriminant Analysis for p = 1

<h4 style="color:red">-----------------------------------------------------------------------------------------------------------------------------</h4>



### The Gaussian density has the form 

[![13.jpg](https://i.postimg.cc/CL2ztgVy/13.jpg)](https://postimg.cc/svpjQbXK)

### Here $\mu_{k}$ is the mean and $\sigma_{k}^2$ the variance (in class k). We will assume that all the $\sigma^2$ = $\sigma$ are the same

### Plugging this into Bayes formula, we get a rather ocmplex expression rof pk(x) = Pr(y=k | X=x)

[![14.jpg](https://i.postimg.cc/ZKmS5sF9/14.jpg)](https://postimg.cc/ThtSk9sG)

<a id='4.4.2'></a>
<h4 style="color:red">-----------------------------------------------------------------------------------------------------------------------------</h4>
<h4 style= "float: right"> <a href="#TOC" >Go to Table of content</a> </h4>

##       4.4.2 Linear Discriminant Analysis for p >1

<h4 style="color:red">-----------------------------------------------------------------------------------------------------------------------------</h4>



<h3>We now extend the LDA classifer to the case of multiple predictors. To
do this, we will assume that X = (X1, X2,...,Xp) is drawn from a multivariate Gaussian (or multivariate normal) distribution, with a class-specifc mean vector and a common covariance matrix. We begin with a brief review of this distribution </h3>

[![15.jpg](https://i.postimg.cc/0NRGTmqq/15.jpg)](https://postimg.cc/mPjFMPDd)

<h3>In the case of p > 1 predictors, the LDA classifer assumes that the
observations in the kth class are drawn from a multivariate Gaussian distribution N(¬µk, Œ£), where ¬µk is a class-specifc mean vector, and Œ£ is a
covariance matrix that is common to all K classes. Plugging the density
function for the kth class, fk(X = x), into (4.15) and performing a little
bit of algebra reveals that the Bayes classifer assigns an observation X to the class for which = x</h3>

[![16.jpg](https://i.postimg.cc/g0Hh1TDv/16.jpg)](https://postimg.cc/qtgg3QWR)

### Result (confusion matrix)

[![19.jpg](https://i.postimg.cc/T2Fz4dPv/19.jpg)](https://postimg.cc/hh8ZJqx2)

## ROC
<h3>The ROC curve is a popular graphic for simultaneously displaying the ROC curve two types of errors for all possible thresholds. The name ‚ÄúROC‚Äù is historic,
and comes from communications theory. It is an acronym for receiver operating characteristics. Figure 4.8 displays the ROC curve for the LDA
classifer on the training data. The overall performance of a classifer, summarized over all possible thresholds, is given by the area under the (ROC)
curve (AUC). An ideal ROC curve will hug the top left corner, so the larger the AUC the better the classifer. For this data the AUC is 0.95, which is close to the maximum of 1.0, so would be considered very good. We expect a classifer that performs no better than chance to have an AUC of 0.5 (when evaluated on an independent test set not used in model training).
ROC curves are useful for comparing diferent classifers, since they take
into account all possible thresholds. It turns out that the ROC curve for
the logistic regression model of Section 4.3.4 ft to these data is virtually
indistinguishable from this one for the LDA model, so we do not display it
here.
</h3>

[![20.jpg](https://i.postimg.cc/9fQgfKfB/20.jpg)](https://postimg.cc/0zh0WtD6)

<h3>A ROC curve for the LDA classifer on the Default data. It
traces out two types of error as we vary the threshold value for the posterior
probability of default. The actual thresholds are not shown. The true positive rate
is the sensitivity: the fraction of defaulters that are correctly identifed, using
a given threshold value. The false positive rate is 1-specifcity: the fraction of
non-defaulters that we classify incorrectly as defaulters, using that same threshold
value. The ideal ROC curve hugs the top left corner, indicating a high true positive
rate and a low false positive rate. The dotted line represents the ‚Äúno information‚Äù
classifer; this is what we would expect if student status and credit card balance
are not associated with probability of default.</h3>

[![21.jpg](https://i.postimg.cc/k4fnbkNX/21.jpg)](https://postimg.cc/Vr0QyZH3)

<a id='4.4.3'></a>
<h4 style="color:red">-----------------------------------------------------------------------------------------------------------------------------</h4>
<h4 style= "float: right"> <a href="#TOC" >Go to Table of content</a> </h4>

##       4.4.3 Quadratic Discriminant Analysis

<h4 style="color:red">-----------------------------------------------------------------------------------------------------------------------------</h4>



<h3>As we have discussed, LDA assumes that the observations within each class
are drawn from a multivariate Gaussian distribution with a class-specifc
mean vector and a covariance matrix that is common to all K classes.
Quadratic discriminant analysis (QDA) provides an alternative approalysis
Like LDA, the QDA classifer results from assuming that the observations
from each class are drawn from a Gaussian distribution, and plugging estimates for the parameters into Bayes‚Äô theorem in order to perform prediction. However, unlike LDA, QDA assumes that each class has its own
covariance matrix. That is, it assumes that an observation from the kth
class is of the form X ‚àº N(¬µk, Œ£k), where Œ£k is a covariance matrix for
the kth class. Under this assumption, the Bayes classifer assigns an observation X = x to the class for which is largest</h3>

<h3>As we have discussed, LDA assumes that the observations within each class
are drawn from a multivariate Gaussian distribution with a class-specifc
mean vector and a covariance matrix that is common to all K classes.
Quadratic discriminant analysis (QDA) provides an alternative approalysis
Like LDA, the QDA classifer results from assuming that the observations
from each class are drawn from a Gaussian distribution, and plugging estimates for the parameters into Bayes‚Äô theorem in order to perform prediction. However, unlike LDA, QDA assumes that each class has its own
covariance matrix. That is, it assumes that an observation from the kth
class is of the form X ‚àº N(¬µk, Œ£k), where Œ£k is a covariance matrix for
the kth class. Under this assumption, the Bayes classifer assigns an observation X = x to the class f is largestor which</h3>

[![22.jpg](https://i.postimg.cc/CxgL9HK0/22.jpg)](https://postimg.cc/nC3JDm7S)

<a id='4.4.4'></a>
<h4 style="color:red">-----------------------------------------------------------------------------------------------------------------------------</h4>
<h4 style= "float: right"> <a href="#TOC" >Go to Table of content</a> </h4>

##       4.4.4 Naive Bayes





<h4 style="color:red">-----------------------------------------------------------------------------------------------------------------------------</h4>



<h3>In previous sections, we used Bayes‚Äô theorem to develop the LDA
and QDA classifiers. Here, we use Bayes‚Äô theorem to motivate the popular
naive Bayes classifier.</h3>

<h3>Recall that Bayes‚Äô theorem provides an expression for the posterior probability pk(x) = Pr(Y = k|X = x) in terms of $\pi_{1}$.......... $\pi_{k}$ and f1(x) ..... fk(x).
To use in practice, we need estimates for $\pi_{1}$.......... $\pi_{k}$ and f1(x) ..... fk(x).
As we saw in previous sections, estimating the prior probabilities $\pi_{1}$.......... $\pi_{k}$ 
    To use in practice, we need estimates for $\pi_{1}$.......... $\pi_{k}$ is typically straightforward: for instance, we can
estimate ÀÜ&k as the proportion.</h3>

<h3>The naive Bayes classifier takes a different tack for estimating f1(x), . . . ,
fK(x). Instead of assuming that these functions belong to a particular
family of distributions (e.g. multivariate normal), we instead make a single
assumption:</h3>

<h4 style= "text-align:center; color:black">Within the kth class, the p predictors are independent.</h4>


<h3>Stated mathematically, this assumption means that for k = 1, . . . ,K,</h3>

<h3 style= "text-align:center; color:blue">$f_{k}(x)$ = $f_{k1}(x1)$ √ó $f_{k2}(x2)$ √ó ¬∑ ¬∑ ¬∑ √ó $f_{kp}(xp)$,</h3>


<h3>where $f_{kj}$ is the density function of the jth predictor among observations
in the kth class.</h3>

<h3>where Why is this assumption so powerful? Essentially, estimating a p-dimensional
density function is challenging because we must consider not only
the marginal distribution of each predictor that is, the distribution of each predictor on its own but also the joint distribution of the predictors that is, the association between the different predictors. </h3>

<h3>In the case of a multivariate normal distribution, the association between the different
predictors is summarized by the off-diagonal elements of the covariance
matrix. However, in general, this association can be very hard to characterize,
and exceedingly challenging to estimate. But by assuming that the
p covariates are independent within each class, we completely eliminate the
need to worry about the association between the p predictors, because we
have simply assumed that there is no association between the predictors!</h3>

<a id='4.5'></a>
<h4 style="color:red">-----------------------------------------------------------------------------------------------------------------------------</h4>
<h4 style= "float: right"> <a href="#TOC" >Go to Table of content</a> </h4>

##       4.5 A Comparison of Classification Methods





<h4 style="color:red">-----------------------------------------------------------------------------------------------------------------------------</h4>



<a id='4.5.1'></a>
<h4 style="color:red">-----------------------------------------------------------------------------------------------------------------------------</h4>
<h4 style= "float: right"> <a href="#TOC" >Go to Table of content</a> </h4>

##       4.5.1 An Analytical Comparison





<h4 style="color:red">-----------------------------------------------------------------------------------------------------------------------------</h4>



<h3>We now perform an analytical (or mathematical) comparison of LDA, QDA,
naive Bayes, and logistic regression. We consider these approaches in a
setting with K classes, so that we assign an observation to the class that
maximizes Pr(Y = k|X = x). Equivalently, we can set K as the baseline
class and assign an observation to the class that maximizes</h3>

[![Capture.jpg](https://i.postimg.cc/13xdc3Mz/Capture.jpg)](https://postimg.cc/zVjpq5sm)

[![11.jpg](https://i.postimg.cc/9F16VhBX/11.jpg)](https://postimg.cc/RN6sQ282)

[![111.jpg](https://i.postimg.cc/vmxkW6wD/111.jpg)](https://postimg.cc/DWFxh0ZK)

[![1111.jpg](https://i.postimg.cc/Ss05VNmd/1111.jpg)](https://postimg.cc/d7B4VFkZ)

<h3>Inspection of (4.32), (4.33), and (4.34) yields the following observations
about LDA, QDA, and naive Bayes:</h3>

[![1.jpg](https://i.postimg.cc/fLwy3s12/1.jpg)](https://postimg.cc/QByhP281)

[![2.jpg](https://i.postimg.cc/8CGP3Dkp/2.jpg)](https://postimg.cc/rDhk0Bt3)

[![3.jpg](https://i.postimg.cc/6q89Rpc0/3.jpg)](https://postimg.cc/m1GfWR0z)

[![4.jpg](https://i.postimg.cc/vH6YZDc8/4.jpg)](https://postimg.cc/fJDnHzc1)

<h3>We close with a brief discussion of K-nearest neighbors (KNN). Recall that KNN takes a completely different approach
from the classifiers seen in this chapter. In order to make a prediction for
an observation X = x, the training observations that are closest to x are
identified. Then X is assigned to the class to which the plurality of these
observations belong. Hence KNN is a completely non-parametric approach:
no assumptions are made about the shape of the decision boundary. We
make the following observations about KNN:</h3>

[![knn.jpg](https://i.postimg.cc/dt72QR27/knn.jpg)](https://postimg.cc/SJh2DMtq)

<a id='4.5.2'></a>
<h4 style="color:red">-----------------------------------------------------------------------------------------------------------------------------</h4>
<h4 style= "float: right"> <a href="#TOC" >Go to Table of content</a> </h4>

##       4.5.2 An Empirical Comparison





<h4 style="color:red">-----------------------------------------------------------------------------------------------------------------------------</h4>



<h3>We now compare the empirical (practical) performance of logistic regression,
LDA, QDA, naive Bayes, and KNN. We generated data from six different
scenarios, each of which involves a binary (two-class) classification
problem. In three of the scenarios, the Bayes decision boundary is linear,
and in the remaining scenarios it is non-linear. For each scenario, we produced
100 random training data sets. On each of these training sets, we
fit each method to the data and computed the resulting test error rate on
a large test set. Results for the linear scenarios are shown in Figure 4.11,
and the results for the non-linear scenarios are in Figure 4.12. The KNN
method requires selection of K, the number of neighbors (not to be confused
with the number of classes in earlier sections of this chapter). We
performed KNN with two values of K: K = 1, and a value of K that was
chosen automatically using an approach called cross-validation, which we
discuss further in Chapter 5. We applied naive Bayes assuming univariate
Gaussian densities for the features within each class (and, of course ‚Äî since
this is the key characteristic of naive Bayes ‚Äî assuming independence of
the features).
In each of the six scenarios, there were p = 2 quantitative predictors.
The scenarios were as follows:</h3>

[![s1.jpg](https://i.postimg.cc/pVjjgVR8/s1.jpg)](https://postimg.cc/XXVX5bZ7)

[![s2.jpg](https://i.postimg.cc/KvyTFX8W/s2.jpg)](https://postimg.cc/6TYqLmRV)

<h3><b style="color:blue">Scenario 1:</b> There were 20 training observations in each of two classes. The
observations within each class were uncorrelated random normal variables
with a different mean in each class. The left-hand panel of Figure 4.11 shows
that LDA performed well in this setting, as one would expect since this is
the model assumed by LDA. Logistic regression also performed quite well,
since it assumes a linear decision boundary. KNN performed poorly because
it paid a price in terms of variance that was not offset by a reduction in bias.
QDA also performed worse than LDA, since it fit a more flexible classifier
than necessary. The performance of naive Bayes was slightly better than
QDA, because the naive Bayes assumption of independent predictors is
correct.</h3>

<h3><b style="color:blue">Scenario 2:</b> Details are as in Scenario 1, except that within each class, the
two predictors had a correlation of ‚àí0.5. The center panel of Figure 4.11
indicates that the performance of most methods is similar to the previous
scenario. The notable exception is naive Bayes, which performs very
poorly here, since the naive Bayes assumption of independent predictors is
violated.</h3>

<h3><b style="color:blue">Scenario 3:</b> As in the previous scenario, there is substantial negative correlation
between the predictors within each class. However, this time we
generated X1 and X2 from the t-distribution, with 50 observations per class.

The t-distribution has a similar shape to the normal distribution, but it has a tendency to yield more extreme points‚Äîthat is, more points that are
far from the mean. In this setting, the decision boundary was still linear,
and so fit into the logistic regression framework. The set-up violated the
assumptions of LDA, since the observations were not drawn from a normal
distribution. The right-hand panel of Figure 4.11 shows that logistic regression
outperformed LDA, though both methods were superior to the other
approaches. In particular, the QDA results deteriorated considerably as a
consequence of non-normality. Naive Bayes performed very poorly because
the independence assumption is violated.
</h3>

<h3><b style="color:blue">Scenario 4:</b> The data were generated from a normal distribution, with a
correlation of 0.5 between the predictors in the first class, and correlation of
‚àí0.5 between the predictors in the second class. This setup corresponded to
the QDA assumption, and resulted in quadratic decision boundaries. The
left-hand panel of Figure 4.12 shows that QDA outperformed all of the other approaches. The naive Bayes assumption of independent predictors
is violated, so naive Bayes performs poorly.</h3>

<h3><b style="color:blue">Scenario 5:</b> The data were generated from a normal distribution with uncorrelated
predictors. Then the responses were sampled from the logistic
function applied to a complicated non-linear function of the predictors. The
center panel of Figure 4.12 shows that both QDA and naive Bayes gave
slightly better results than the linear methods, while the much more flexible
KNN-CV method gave the best results. But KNN with K = 1 gave the
worst results out of all methods. This highlights the fact that even when the
data exhibits a complex non-linear relationship, a non-parametric method
such as KNN can still give poor results if the level of smoothness is not
chosen correctly.</h3>

<h3><b style="color:blue">Scenario 6:</b> The observations were generated from a normal distribution
with a different diagonal covariance matrix for each class. However, the
sample size was very small: just n = 6 in each class. Naive Bayes performed
very well, because its assumptions are met. LDA and logistic regression
performed poorly because the true decision boundary is non-linear, due to
the unequal covariance matrices. QDA performed a bit worse than naive
Bayes, because given the very small sample size, the former incurred too
much variance in estimating the correlation between the predictors within
each class. KNN‚Äôs performance also suffered due to the very small sample
size.</h3>

<h3 style="color:blue">These six examples illustrate that no one method will dominate the others
in every situation. When the true decision boundaries are linear, then
the LDA and logistic regression approaches will tend to perform well. When
the boundaries are moderately non-linear, QDA or naive Bayes may give
better results. Finally, for much more complicated decision boundaries, a
non-parametric approach such as KNN can be superior. But the level of
smoothness for a non-parametric approach must be chosen carefully. In the
next chapter we examine a number of approaches for choosing the correct
level of smoothness and, in general, for selecting the best overall method.
Finally, recall from Chapter 3 that in the regression setting we can accommodate
a non-linear relationship between the predictors and the response
by performing regression using transformations of the predictors. A similar
approach could be taken in the classification setting:</h3>

<h3 style="color:blue">For instance, we could create a more flexible version of logistic regression by including X2, X3,
and even X4 as predictors. This may or may not improve logistic regression‚Äôs
performance, depending on whether the increase in variance due to
the added flexibility is offset by a sufficiently large reduction in bias. We
could do the same for LDA. If we added all possible quadratic terms and
cross-products to LDA, the form of the model would be the same as the
QDA model, although the parameter estimates would be different. This
device allows us to move somewhere between an LDA and a QDA model.</h3>

<a id='4.6'></a>
<h4 style="color:red">-----------------------------------------------------------------------------------------------------------------------------</h4>
<h4 style= "float: right"> <a href="#TOC" >Go to Table of content</a> </h4>

##       4.6 Generalized Linear Models





<h4 style="color:red">-----------------------------------------------------------------------------------------------------------------------------</h4>



<h3>In Chapter 3, we assumed that the response Y is quantitative, and explored
the use of least squares linear regression to predict Y . Thus far in
this chapter, we have instead assumed that Y is qualitative. However, we
may sometimes be faced with situations in which Y is neither qualitative
nor quantitative, and so neither linear regression from Chapter 3 nor the
classification approaches covered in this chapter is applicable.</h3>

<h3>As a concrete example, we consider the Bikeshare data set. The response
is bikers, the number of hourly users of a bike sharing program in Washington,
DC. This response value is neither qualitative nor quantitative:
instead, it takes on non-negative integer values, or counts. We will consider predicting bikers using the covariates mnth (month of the year), hr (hour
of the day, from 0 to 23), workingday (an indicator variable that equals 1 if
it is neither a weekend nor a holiday), temp (the normalized temperature,
in Celsius), and weathersit (a qualitative variable that takes on one of four
possible values: clear; misty or cloudy; light rain or light snow; or heavy
rain or heavy snow.)
In the analyses that follow, we will treat (mnth), (hr), and (weathersit) as
qualitative variables.</h3>

<a id='4.6.1'></a>
<h4 style="color:red">-----------------------------------------------------------------------------------------------------------------------------</h4>
<h4 style= "float: right"> <a href="#TOC" >Go to Table of content</a> </h4>

##       4.6.1 Linear Regression on the Bikeshare Data





<h4 style="color:red">-----------------------------------------------------------------------------------------------------------------------------</h4>



<h3 style= "text-align:center; color:blue">Results for a least squares linear model fit to predict bikers in
the Bikeshare data.</h3>


[![l1.jpg](https://i.postimg.cc/Hxwy83Sz/l1.jpg)](https://postimg.cc/sQ2XtPCG)

[![l2.jpg](https://i.postimg.cc/XqD7rC0x/l2.jpg)](https://postimg.cc/T5gXSh1W)

<h3 style= "text-align:center; color:blue">Mean/Variance Relationship</h3>


[![l3.jpg](https://i.postimg.cc/5tnc9MJS/l3.jpg)](https://postimg.cc/mczpVJtP)

<h3><ul>
    <li>In the left plot we aee that the variance mostly increase with the mean</li>
    <li>10% of the linear model predictions are negative. (not shown here)</li>
    <li>Taking the log(Bikers) alleviates this, but has its own problems: e.g predictions are on the wrong scale and sum counts are zeros</li>
</ul></h3>

<a id='4.6.2'></a>
<h4 style="color:red">-----------------------------------------------------------------------------------------------------------------------------</h4>
<h4 style= "float: right"> <a href="#TOC" >Go to Table of content</a> </h4>

##       4.6.2 Poisson Regression on the Bikeshare Data





<h4 style="color:red">-----------------------------------------------------------------------------------------------------------------------------</h4>



<h3>Poisson Distribution is useful for modeling counts:</h3>

[![pr.jpg](https://i.postimg.cc/y69H9tys/pr.jpg)](https://postimg.cc/6T9gxHPm)

[![pr2.jpg](https://i.postimg.cc/s2vF2N2F/pr2.jpg)](https://postimg.cc/VSQZG4M4)

[![prt.jpg](https://i.postimg.cc/66CkTRBQ/prt.jpg)](https://postimg.cc/qgMb5Nhf)

[![prl.jpg](https://i.postimg.cc/Qddw6y1B/prl.jpg)](https://postimg.cc/bs4mrL68)

<a id='4.6.3'></a>
<h4 style="color:red">-----------------------------------------------------------------------------------------------------------------------------</h4>
<h4 style= "float: right"> <a href="#TOC" >Go to Table of content</a> </h4>

##       4.6.3 Generalized Linear Models in Greater Generality




<h4 style="color:red">-----------------------------------------------------------------------------------------------------------------------------</h4>



[![gm.jpg](https://i.postimg.cc/cHyDN9gB/gm.jpg)](https://postimg.cc/NyD86xxK)


```python

```


```python

```


```python

```


```python

```


```python

```


```python
<h3></h3>
```


```python
<h3></h3>
```


```python
<h3></h3>
```


```python
<h3></h3>
```


```python
<h3></h3>
```

[![image.jpg](https://i.postimg.cc/kG5g9Cdb/image.jpg)](https://postimg.cc/hzHBbHvD)


```python
!pip install --upgrade notebook nbconvert jupyter_contrib_nbextensions

```

    Requirement already satisfied: notebook in c:\users\muham\anaconda3\envs\islp\lib\site-packages (7.2.2)
    Requirement already satisfied: nbconvert in c:\users\muham\anaconda3\envs\islp\lib\site-packages (7.16.4)
    Requirement already satisfied: jupyter_contrib_nbextensions in c:\users\muham\anaconda3\envs\islp\lib\site-packages (0.7.0)
    Requirement already satisfied: jupyter-server<3,>=2.4.0 in c:\users\muham\anaconda3\envs\islp\lib\site-packages (from notebook) (2.14.2)
    Requirement already satisfied: jupyterlab-server<3,>=2.27.1 in c:\users\muham\anaconda3\envs\islp\lib\site-packages (from notebook) (2.27.3)
    Requirement already satisfied: jupyterlab<4.3,>=4.2.0 in c:\users\muham\anaconda3\envs\islp\lib\site-packages (from notebook) (4.2.5)
    Requirement already satisfied: notebook-shim<0.3,>=0.2 in c:\users\muham\anaconda3\envs\islp\lib\site-packages (from notebook) (0.2.4)
    Requirement already satisfied: tornado>=6.2.0 in c:\users\muham\anaconda3\envs\islp\lib\site-packages (from notebook) (6.4.1)
    Requirement already satisfied: beautifulsoup4 in c:\users\muham\anaconda3\envs\islp\lib\site-packages (from nbconvert) (4.12.3)
    Requirement already satisfied: bleach!=5.0.0 in c:\users\muham\anaconda3\envs\islp\lib\site-packages (from nbconvert) (6.1.0)
    Requirement already satisfied: defusedxml in c:\users\muham\anaconda3\envs\islp\lib\site-packages (from nbconvert) (0.7.1)
    Requirement already satisfied: importlib-metadata>=3.6 in c:\users\muham\anaconda3\envs\islp\lib\site-packages (from nbconvert) (8.0.0)
    Requirement already satisfied: jinja2>=3.0 in c:\users\muham\anaconda3\envs\islp\lib\site-packages (from nbconvert) (3.1.4)
    Requirement already satisfied: jupyter-core>=4.7 in c:\users\muham\anaconda3\envs\islp\lib\site-packages (from nbconvert) (5.7.2)
    Requirement already satisfied: jupyterlab-pygments in c:\users\muham\anaconda3\envs\islp\lib\site-packages (from nbconvert) (0.3.0)
    Requirement already satisfied: markupsafe>=2.0 in c:\users\muham\anaconda3\envs\islp\lib\site-packages (from nbconvert) (2.1.5)
    Requirement already satisfied: mistune<4,>=2.0.3 in c:\users\muham\anaconda3\envs\islp\lib\site-packages (from nbconvert) (3.0.2)
    Requirement already satisfied: nbclient>=0.5.0 in c:\users\muham\anaconda3\envs\islp\lib\site-packages (from nbconvert) (0.10.0)
    Requirement already satisfied: nbformat>=5.7 in c:\users\muham\anaconda3\envs\islp\lib\site-packages (from nbconvert) (5.10.4)
    Requirement already satisfied: packaging in c:\users\muham\anaconda3\envs\islp\lib\site-packages (from nbconvert) (24.1)
    Requirement already satisfied: pandocfilters>=1.4.1 in c:\users\muham\anaconda3\envs\islp\lib\site-packages (from nbconvert) (1.5.1)
    Requirement already satisfied: pygments>=2.4.1 in c:\users\muham\anaconda3\envs\islp\lib\site-packages (from nbconvert) (2.18.0)
    Requirement already satisfied: tinycss2 in c:\users\muham\anaconda3\envs\islp\lib\site-packages (from nbconvert) (1.3.0)
    Requirement already satisfied: traitlets>=5.1 in c:\users\muham\anaconda3\envs\islp\lib\site-packages (from nbconvert) (5.14.3)
    Requirement already satisfied: ipython-genutils in c:\users\muham\anaconda3\envs\islp\lib\site-packages (from jupyter_contrib_nbextensions) (0.2.0)
    Requirement already satisfied: jupyter-contrib-core>=0.3.3 in c:\users\muham\anaconda3\envs\islp\lib\site-packages (from jupyter_contrib_nbextensions) (0.4.2)
    Requirement already satisfied: jupyter-highlight-selected-word>=0.1.1 in c:\users\muham\anaconda3\envs\islp\lib\site-packages (from jupyter_contrib_nbextensions) (0.2.0)
    Requirement already satisfied: jupyter-nbextensions-configurator>=0.4.0 in c:\users\muham\anaconda3\envs\islp\lib\site-packages (from jupyter_contrib_nbextensions) (0.6.4)
    Requirement already satisfied: lxml in c:\users\muham\anaconda3\envs\islp\lib\site-packages (from jupyter_contrib_nbextensions) (5.2.2)
    Requirement already satisfied: six>=1.9.0 in c:\users\muham\anaconda3\envs\islp\lib\site-packages (from bleach!=5.0.0->nbconvert) (1.16.0)
    Requirement already satisfied: webencodings in c:\users\muham\anaconda3\envs\islp\lib\site-packages (from bleach!=5.0.0->nbconvert) (0.5.1)
    Requirement already satisfied: zipp>=0.5 in c:\users\muham\anaconda3\envs\islp\lib\site-packages (from importlib-metadata>=3.6->nbconvert) (3.19.2)
    Requirement already satisfied: setuptools in c:\users\muham\anaconda3\envs\islp\lib\site-packages (from jupyter-contrib-core>=0.3.3->jupyter_contrib_nbextensions) (69.5.1)
    Requirement already satisfied: platformdirs>=2.5 in c:\users\muham\anaconda3\envs\islp\lib\site-packages (from jupyter-core>=4.7->nbconvert) (4.2.2)
    Requirement already satisfied: pywin32>=300 in c:\users\muham\anaconda3\envs\islp\lib\site-packages (from jupyter-core>=4.7->nbconvert) (306)
    Requirement already satisfied: pyyaml in c:\users\muham\anaconda3\envs\islp\lib\site-packages (from jupyter-nbextensions-configurator>=0.4.0->jupyter_contrib_nbextensions) (6.0.1)
    Requirement already satisfied: anyio>=3.1.0 in c:\users\muham\anaconda3\envs\islp\lib\site-packages (from jupyter-server<3,>=2.4.0->notebook) (4.4.0)
    Requirement already satisfied: argon2-cffi>=21.1 in c:\users\muham\anaconda3\envs\islp\lib\site-packages (from jupyter-server<3,>=2.4.0->notebook) (23.1.0)
    Requirement already satisfied: jupyter-client>=7.4.4 in c:\users\muham\anaconda3\envs\islp\lib\site-packages (from jupyter-server<3,>=2.4.0->notebook) (8.6.2)
    Requirement already satisfied: jupyter-events>=0.9.0 in c:\users\muham\anaconda3\envs\islp\lib\site-packages (from jupyter-server<3,>=2.4.0->notebook) (0.10.0)
    Requirement already satisfied: jupyter-server-terminals>=0.4.4 in c:\users\muham\anaconda3\envs\islp\lib\site-packages (from jupyter-server<3,>=2.4.0->notebook) (0.5.3)
    Requirement already satisfied: overrides>=5.0 in c:\users\muham\anaconda3\envs\islp\lib\site-packages (from jupyter-server<3,>=2.4.0->notebook) (7.7.0)
    Requirement already satisfied: prometheus-client>=0.9 in c:\users\muham\anaconda3\envs\islp\lib\site-packages (from jupyter-server<3,>=2.4.0->notebook) (0.20.0)
    Requirement already satisfied: pywinpty>=2.0.1 in c:\users\muham\anaconda3\envs\islp\lib\site-packages (from jupyter-server<3,>=2.4.0->notebook) (2.0.13)
    Requirement already satisfied: pyzmq>=24 in c:\users\muham\anaconda3\envs\islp\lib\site-packages (from jupyter-server<3,>=2.4.0->notebook) (26.2.0)
    Requirement already satisfied: send2trash>=1.8.2 in c:\users\muham\anaconda3\envs\islp\lib\site-packages (from jupyter-server<3,>=2.4.0->notebook) (1.8.3)
    Requirement already satisfied: terminado>=0.8.3 in c:\users\muham\anaconda3\envs\islp\lib\site-packages (from jupyter-server<3,>=2.4.0->notebook) (0.18.1)
    Requirement already satisfied: websocket-client>=1.7 in c:\users\muham\anaconda3\envs\islp\lib\site-packages (from jupyter-server<3,>=2.4.0->notebook) (1.8.0)
    Requirement already satisfied: async-lru>=1.0.0 in c:\users\muham\anaconda3\envs\islp\lib\site-packages (from jupyterlab<4.3,>=4.2.0->notebook) (2.0.4)
    Requirement already satisfied: httpx>=0.25.0 in c:\users\muham\anaconda3\envs\islp\lib\site-packages (from jupyterlab<4.3,>=4.2.0->notebook) (0.27.2)
    Requirement already satisfied: ipykernel>=6.5.0 in c:\users\muham\anaconda3\envs\islp\lib\site-packages (from jupyterlab<4.3,>=4.2.0->notebook) (6.29.5)
    Requirement already satisfied: jupyter-lsp>=2.0.0 in c:\users\muham\anaconda3\envs\islp\lib\site-packages (from jupyterlab<4.3,>=4.2.0->notebook) (2.2.5)
    Requirement already satisfied: tomli>=1.2.2 in c:\users\muham\anaconda3\envs\islp\lib\site-packages (from jupyterlab<4.3,>=4.2.0->notebook) (2.0.1)
    Requirement already satisfied: babel>=2.10 in c:\users\muham\anaconda3\envs\islp\lib\site-packages (from jupyterlab-server<3,>=2.27.1->notebook) (2.16.0)
    Requirement already satisfied: json5>=0.9.0 in c:\users\muham\anaconda3\envs\islp\lib\site-packages (from jupyterlab-server<3,>=2.27.1->notebook) (0.9.25)
    Requirement already satisfied: jsonschema>=4.18.0 in c:\users\muham\anaconda3\envs\islp\lib\site-packages (from jupyterlab-server<3,>=2.27.1->notebook) (4.23.0)
    Requirement already satisfied: requests>=2.31 in c:\users\muham\anaconda3\envs\islp\lib\site-packages (from jupyterlab-server<3,>=2.27.1->notebook) (2.32.3)
    Requirement already satisfied: fastjsonschema>=2.15 in c:\users\muham\anaconda3\envs\islp\lib\site-packages (from nbformat>=5.7->nbconvert) (2.20.0)
    Requirement already satisfied: soupsieve>1.2 in c:\users\muham\anaconda3\envs\islp\lib\site-packages (from beautifulsoup4->nbconvert) (2.6)
    Requirement already satisfied: idna>=2.8 in c:\users\muham\anaconda3\envs\islp\lib\site-packages (from anyio>=3.1.0->jupyter-server<3,>=2.4.0->notebook) (3.7)
    Requirement already satisfied: sniffio>=1.1 in c:\users\muham\anaconda3\envs\islp\lib\site-packages (from anyio>=3.1.0->jupyter-server<3,>=2.4.0->notebook) (1.3.1)
    Requirement already satisfied: exceptiongroup>=1.0.2 in c:\users\muham\anaconda3\envs\islp\lib\site-packages (from anyio>=3.1.0->jupyter-server<3,>=2.4.0->notebook) (1.2.2)
    Requirement already satisfied: typing-extensions>=4.1 in c:\users\muham\anaconda3\envs\islp\lib\site-packages (from anyio>=3.1.0->jupyter-server<3,>=2.4.0->notebook) (4.12.2)
    Requirement already satisfied: argon2-cffi-bindings in c:\users\muham\anaconda3\envs\islp\lib\site-packages (from argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->notebook) (21.2.0)
    Requirement already satisfied: certifi in c:\users\muham\anaconda3\envs\islp\lib\site-packages (from httpx>=0.25.0->jupyterlab<4.3,>=4.2.0->notebook) (2024.7.4)
    Requirement already satisfied: httpcore==1.* in c:\users\muham\anaconda3\envs\islp\lib\site-packages (from httpx>=0.25.0->jupyterlab<4.3,>=4.2.0->notebook) (1.0.5)
    Requirement already satisfied: h11<0.15,>=0.13 in c:\users\muham\anaconda3\envs\islp\lib\site-packages (from httpcore==1.*->httpx>=0.25.0->jupyterlab<4.3,>=4.2.0->notebook) (0.14.0)
    Requirement already satisfied: comm>=0.1.1 in c:\users\muham\anaconda3\envs\islp\lib\site-packages (from ipykernel>=6.5.0->jupyterlab<4.3,>=4.2.0->notebook) (0.2.2)
    Requirement already satisfied: debugpy>=1.6.5 in c:\users\muham\anaconda3\envs\islp\lib\site-packages (from ipykernel>=6.5.0->jupyterlab<4.3,>=4.2.0->notebook) (1.8.5)
    Requirement already satisfied: ipython>=7.23.1 in c:\users\muham\anaconda3\envs\islp\lib\site-packages (from ipykernel>=6.5.0->jupyterlab<4.3,>=4.2.0->notebook) (8.18.1)
    Requirement already satisfied: matplotlib-inline>=0.1 in c:\users\muham\anaconda3\envs\islp\lib\site-packages (from ipykernel>=6.5.0->jupyterlab<4.3,>=4.2.0->notebook) (0.1.7)
    Requirement already satisfied: nest-asyncio in c:\users\muham\anaconda3\envs\islp\lib\site-packages (from ipykernel>=6.5.0->jupyterlab<4.3,>=4.2.0->notebook) (1.6.0)
    Requirement already satisfied: psutil in c:\users\muham\anaconda3\envs\islp\lib\site-packages (from ipykernel>=6.5.0->jupyterlab<4.3,>=4.2.0->notebook) (6.0.0)
    Requirement already satisfied: attrs>=22.2.0 in c:\users\muham\anaconda3\envs\islp\lib\site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->notebook) (23.2.0)
    Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\users\muham\anaconda3\envs\islp\lib\site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->notebook) (2023.12.1)
    Requirement already satisfied: referencing>=0.28.4 in c:\users\muham\anaconda3\envs\islp\lib\site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->notebook) (0.35.1)
    Requirement already satisfied: rpds-py>=0.7.1 in c:\users\muham\anaconda3\envs\islp\lib\site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->notebook) (0.20.0)
    Requirement already satisfied: python-dateutil>=2.8.2 in c:\users\muham\anaconda3\envs\islp\lib\site-packages (from jupyter-client>=7.4.4->jupyter-server<3,>=2.4.0->notebook) (2.9.0.post0)
    Requirement already satisfied: python-json-logger>=2.0.4 in c:\users\muham\anaconda3\envs\islp\lib\site-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook) (2.0.7)
    Requirement already satisfied: rfc3339-validator in c:\users\muham\anaconda3\envs\islp\lib\site-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook) (0.1.4)
    Requirement already satisfied: rfc3986-validator>=0.1.1 in c:\users\muham\anaconda3\envs\islp\lib\site-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook) (0.1.1)
    Requirement already satisfied: charset-normalizer<4,>=2 in c:\users\muham\anaconda3\envs\islp\lib\site-packages (from requests>=2.31->jupyterlab-server<3,>=2.27.1->notebook) (3.3.2)
    Requirement already satisfied: urllib3<3,>=1.21.1 in c:\users\muham\anaconda3\envs\islp\lib\site-packages (from requests>=2.31->jupyterlab-server<3,>=2.27.1->notebook) (2.2.2)
    Requirement already satisfied: decorator in c:\users\muham\anaconda3\envs\islp\lib\site-packages (from ipython>=7.23.1->ipykernel>=6.5.0->jupyterlab<4.3,>=4.2.0->notebook) (5.1.1)
    Requirement already satisfied: jedi>=0.16 in c:\users\muham\anaconda3\envs\islp\lib\site-packages (from ipython>=7.23.1->ipykernel>=6.5.0->jupyterlab<4.3,>=4.2.0->notebook) (0.19.1)
    Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in c:\users\muham\anaconda3\envs\islp\lib\site-packages (from ipython>=7.23.1->ipykernel>=6.5.0->jupyterlab<4.3,>=4.2.0->notebook) (3.0.47)
    Requirement already satisfied: stack-data in c:\users\muham\anaconda3\envs\islp\lib\site-packages (from ipython>=7.23.1->ipykernel>=6.5.0->jupyterlab<4.3,>=4.2.0->notebook) (0.6.3)
    Requirement already satisfied: colorama in c:\users\muham\anaconda3\envs\islp\lib\site-packages (from ipython>=7.23.1->ipykernel>=6.5.0->jupyterlab<4.3,>=4.2.0->notebook) (0.4.6)
    Requirement already satisfied: fqdn in c:\users\muham\anaconda3\envs\islp\lib\site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook) (1.5.1)
    Requirement already satisfied: isoduration in c:\users\muham\anaconda3\envs\islp\lib\site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook) (20.11.0)
    Requirement already satisfied: jsonpointer>1.13 in c:\users\muham\anaconda3\envs\islp\lib\site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook) (3.0.0)
    Requirement already satisfied: uri-template in c:\users\muham\anaconda3\envs\islp\lib\site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook) (1.3.0)
    Requirement already satisfied: webcolors>=24.6.0 in c:\users\muham\anaconda3\envs\islp\lib\site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook) (24.8.0)
    Requirement already satisfied: cffi>=1.0.1 in c:\users\muham\anaconda3\envs\islp\lib\site-packages (from argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->notebook) (1.17.0)
    Requirement already satisfied: pycparser in c:\users\muham\anaconda3\envs\islp\lib\site-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->notebook) (2.22)
    Requirement already satisfied: parso<0.9.0,>=0.8.3 in c:\users\muham\anaconda3\envs\islp\lib\site-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel>=6.5.0->jupyterlab<4.3,>=4.2.0->notebook) (0.8.4)
    Requirement already satisfied: wcwidth in c:\users\muham\anaconda3\envs\islp\lib\site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=7.23.1->ipykernel>=6.5.0->jupyterlab<4.3,>=4.2.0->notebook) (0.2.13)
    Requirement already satisfied: arrow>=0.15.0 in c:\users\muham\anaconda3\envs\islp\lib\site-packages (from isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook) (1.3.0)
    Requirement already satisfied: executing>=1.2.0 in c:\users\muham\anaconda3\envs\islp\lib\site-packages (from stack-data->ipython>=7.23.1->ipykernel>=6.5.0->jupyterlab<4.3,>=4.2.0->notebook) (2.1.0)
    Requirement already satisfied: asttokens>=2.1.0 in c:\users\muham\anaconda3\envs\islp\lib\site-packages (from stack-data->ipython>=7.23.1->ipykernel>=6.5.0->jupyterlab<4.3,>=4.2.0->notebook) (2.4.1)
    Requirement already satisfied: pure-eval in c:\users\muham\anaconda3\envs\islp\lib\site-packages (from stack-data->ipython>=7.23.1->ipykernel>=6.5.0->jupyterlab<4.3,>=4.2.0->notebook) (0.2.3)
    Requirement already satisfied: types-python-dateutil>=2.8.10 in c:\users\muham\anaconda3\envs\islp\lib\site-packages (from arrow>=0.15.0->isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook) (2.9.0.20240821)
    


```python
!jupyter nbconvert --to markdown ITSL.ipynb
```

    Traceback (most recent call last):
      File "C:\Users\muham\anaconda3\envs\islp\lib\runpy.py", line 197, in _run_module_as_main
        return _run_code(code, main_globals, None,
      File "C:\Users\muham\anaconda3\envs\islp\lib\runpy.py", line 87, in _run_code
        exec(code, run_globals)
      File "C:\Users\muham\anaconda3\envs\islp\Scripts\jupyter-nbconvert.EXE\__main__.py", line 4, in <module>
      File "C:\Users\muham\anaconda3\envs\islp\lib\site-packages\nbconvert\nbconvertapp.py", line 193, in <module>
        class NbConvertApp(JupyterApp):
      File "C:\Users\muham\anaconda3\envs\islp\lib\site-packages\nbconvert\nbconvertapp.py", line 252, in NbConvertApp
        Options include {get_export_names()}.
      File "C:\Users\muham\anaconda3\envs\islp\lib\site-packages\nbconvert\exporters\base.py", line 145, in get_export_names
        e = get_exporter(exporter_name)(config=config)
      File "C:\Users\muham\anaconda3\envs\islp\lib\site-packages\nbconvert\exporters\base.py", line 106, in get_exporter
        exporter = items[0].load()
      File "C:\Users\muham\anaconda3\envs\islp\lib\site-packages\importlib_metadata\__init__.py", line 183, in load
        module = import_module(match.group('module'))
      File "C:\Users\muham\anaconda3\envs\islp\lib\importlib\__init__.py", line 127, in import_module
        return _bootstrap._gcd_import(name[level:], package, level)
      File "C:\Users\muham\anaconda3\envs\islp\lib\site-packages\jupyter_contrib_nbextensions\nbconvert_support\__init__.py", line 5, in <module>
        from .collapsible_headings import ExporterCollapsibleHeadings
      File "C:\Users\muham\anaconda3\envs\islp\lib\site-packages\jupyter_contrib_nbextensions\nbconvert_support\collapsible_headings.py", line 6, in <module>
        from notebook.services.config import ConfigManager
    ModuleNotFoundError: No module named 'notebook.services'
    


```python

```


```python

```


```python

```


```python

```


```python

```


```python

```


```python

```


```python

```


```python

```


```python

```


```python

```


```python

```


```python

```


```python

```


```python

```


```python

```


```python

```


```python

```


```python

```

here \(\text{RSS}\) is the residual sum of squares.

## Confidence Intervals

Standard errors can be used to compute confidence intervals. A 95% confidence interval gives a range of values within which we expect the true parameter value to lie with 95% probability.

### Confidence Interval for Slope (\(\hat{\beta_1}\))

For the slope (\(\hat{\beta_1}\)), the 95% confidence interval is approximately:

\[ \hat{\beta_1} \pm 2 \cdot \text{SE}(\hat{\beta_1}) \]

That is, there is approximately a 95% chance that the interval:

\[ \hat{\beta_1} - 2 \cdot \text{SE}(\hat{\beta_1}) \quad \text{to} \quad \hat{\beta_1} + 2 \cdot \text{SE}(\hat{\beta_1}) \]

will contain the true value of \(\beta_1\).

### Confidence Interval for Intercept (\(\hat{\beta_0}\))

Similarly, for the intercept (\(\hat{\beta_0}\)), the 95% confidence interval is:

\[ \hat{\beta_0} \pm 2 \cdot \text{SE}(\hat{\beta_0}) \]

### Summary

- **Standard Errors**: Measure the accuracy of the regression coefficient estimates.
- **Residual Standard Error (RSE)**: An estimate of the standard deviation of the error term.
- **Confidence Intervals**: Provide a range within which the true parameter value is expected to lie with a certain probability (e.g., 95%).

These concepts and calculations help you understand the precision and reliability of your regression model's estimates.



```python

```


```python

```

# Understanding Regression Equation and Variance with Example

## The Regression Equation

\[ Y = 2 + 3X + \epsilon \]

- **\( Y \)**: The outcome or dependent variable you are trying to predict.
- **2**: The intercept, which is the value of \( Y \) when \( X \) is 0.
- **3**: The slope, which indicates that for each unit increase in \( X \), \( Y \) increases by 3 units.
- **\( \epsilon \)**: The error term, which accounts for the variability in \( Y \) that cannot be explained by the linear relationship with \( X \).

## Variance and Standard Error

### Variance of the Estimate of the Mean (\( \hat{\mu} \)) and Its Standard Error

\[ \text{Var}(\hat{\mu}) = \text{SE}(\hat{\mu})^2 = \frac{\sigma^2}{n} \]

- **\( \sigma \)**: The standard deviation of the actual \( Y \) values.
- **\( n \)**: The number of observations (data points).
- **\(\text{SE}(\hat{\mu})\)**: The standard error of the estimated mean (\(\hat{\mu}\)), which tells us how much \(\hat{\mu}\) is expected to vary from the actual mean (\(\mu\)).

## Explanation of the Photo

The photo contains detailed mathematical expressions to explain the variability and standard error associated with regression coefficients:

- **\(\sigma\)**: The standard deviation of the actual values of \( Y \).
- **\(\hat{\beta_0}\)** and **\(\hat{\beta_1}\)**: Estimates of the regression coefficients (intercept and slope).
- **\(\text{SE}(\hat{\beta_0})\)** and **\(\text{SE}(\hat{\beta_1})\)**: Standard errors of these estimates, which measure how much these estimates are expected to vary from the true values (\(\beta_0\) and \(\beta_1\)).

### Key Points from the Photo

1. **Variance of \(\hat{\mu}\)**:
   \[
   \text{Var}(\hat{\mu}) = \text{SE}(\hat{\mu})^2 = \frac{\sigma^2}{n}
   \]
   - The variance of the estimated mean (\(\hat{\mu}\)) decreases as the number of observations (\(n\)) increases. This means more data leads to a more precise estimate.

2. **Standard Errors of \(\hat{\beta_0}\) and \(\hat{\beta_1}\)**:
   - The standard error of \(\hat{\beta_0}\) (intercept) is given by:
     \[
     \text{SE}(\hat{\beta_0})^2 = \sigma^2 \left[ \frac{1}{n} + \frac{\bar{X}^2}{\sum_{i=1}^n (X_i - \bar{X})^2} \right]
     \]
   - The standard error of \(\hat{\beta_1}\) (slope) is given by:
     \[
     \text{SE}(\hat{\beta_1})^2 = \frac{\sigma^2}{\sum_{i=1}^n (X_i - \bar{X})^2}
     \]

## Simplified Explanation with Example

Imagine you want to predict someone's height (\( Y \)) based on their shoe size (\( X \)). You have collected data from several people.

- The regression equation you found is \( Y = 2 + 3X + \epsilon \).
  - This means if someone has a shoe size of 0, their height would start at 2 units (e.g., 2 inches, but not realistic in practice).
  - For each increase in shoe size by 1 unit, the height increases by 3 units.

### Example Dataset

| Data Point | Shoe Size (\( X \)) | Height (\( Y \)) |
|------------|----------------------|------------------|
| 1          | 1                    | 5                |
| 2          | 2                    | 8                |
| 3          | 3                    | 11               |
| 4          | 4                    | 14               |

### Calculation of Standard Errors

Using the given formulas, you can calculate the standard errors of the regression coefficients (\( \hat{\beta_0} \) and \( \hat{\beta_1} \)) and the estimated mean (\( \hat{\mu} \)).

- **More Data**: Reduces the standard error, making your estimates more reliable.

### Summary

- **Intercept (\(\beta_0\))**: This is the starting value of \( Y \) when \( X \) is zero.
- **Slope (\(\beta_1\))**: This tells you how much \( Y \) changes for a one-unit change in \( X \).
- **Standard Error**: A measure of the accuracy of the coefficients' estimates.
- **More Data**: Reduces the standard error, making your estimates more reliable.

In summary, these equations and concepts help you understand how accurate your regression model is and how much uncertainty there is in your estimates of the coefficients. More data typically leads to more precise estimates.



```python

```
